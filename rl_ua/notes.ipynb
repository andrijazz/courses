{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly readings\n",
    "\n",
    "c1.w1. Chapter 2-2.7 (Pages 25-36)\n",
    "\n",
    "c1.w2. Chapter 3.3 (pages 47-56)\n",
    "\n",
    "c1.w3. Chapter 3.5 - 3.8 (pages 58-67)\n",
    "\n",
    "c1.w4. Chapter 4.1, 4.2, 4.3, 4.4, 4.6, 4.7 (pages 73-88)\n",
    "\n",
    "c2.w1. None\n",
    "\n",
    "c2.w2. Chapter 5.0-5.5 (pp. 91-104) in the Reinforcement Learning textbook\n",
    "\n",
    "c2.w3. Chapter 6.3 (pp. 116-128)\n",
    "\n",
    "c2.w4. Chapter 6.4-6.6 (pp. 129-134)\n",
    "\n",
    "c2.w5. Chapter 8.1-8.3 (pp. 159-166) (For a summary of what we've covered in the specialization so far, read: pp. 189-191)\n",
    "\n",
    "c3.w1. Ch. 9.1 - 9.4 (pp. 197- 209) in the Reinforcement Learning textbook.\n",
    "\n",
    "c3.w2. Ch. 9.4 - 9.5.0 (pp. 204 - 210), 9.5.3 - 9.5.4 (pp 215 - 222) and 9.7 (pg 223 - 228) in the Reinforcement Learning textbook.\n",
    "\n",
    "c3.w3. Chapter 10 (pp. 243-246) & 10.3 (pp. 249-252) in the Reinforcement Learning textbook. Note: skip over section 10.2.\n",
    "\n",
    "\n",
    "\n",
    "c3.w4.\n",
    "\n",
    "c4.w1.\n",
    "\n",
    "c4.w2.\n",
    "\n",
    "c4.w3.\n",
    "\n",
    "c4.w4.\n",
    "\n",
    "c4.w5.\n",
    "\n",
    "c4.w6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1. Fundamentals of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1. The K-Armed Bandit Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.1.\n",
    "\n",
    "Greedy action will be selected in 50% of the cases. In other 50% we will have a choise between greedy and non-greedy action. That gives us $0.5 + 0.5 * 0.5 = 0.75$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2. Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3. Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4. Dynamic Programming\n",
    "\n",
    "**Dynamic programming** is the method for solving complex problems by 1. breaking them down into subproblems -> 2. solve the subproblems -> 3. combine solutions to subproblems\n",
    "\n",
    "Dynamic Programming is a very general solution method for problems which have two properties:\n",
    "1. Optimal substructure\n",
    "    * Principle of optimality applies\n",
    "    * Optimal solution can be decomposed into subproblems\n",
    "2. Overlapping subproblems\n",
    "    * Subproblems recur many times\n",
    "    * Solutions can be cached and reused\n",
    "    \n",
    "MDPs satisfy both properties - Bellman equation gives recursive decomposition and value function stores and reuses solutions (value function is the *cache*).\n",
    "\n",
    "_def_. **Policy evaluation** is the task of determening state-value function for a specific policy\n",
    "\n",
    "<img src=\"imgs/policy_eval.png\">\n",
    "\n",
    "_def_. **Control** is the task of finding the policy to obtain as much as reward as possible - finding the policy which maximises the value function.\n",
    "\n",
    "Control is a ultimate goal of Reinforcement learning but the task of policy evaluation is the necessary first step. This week we will be looking at collection of algorithms used for solving policy evaluation and control tasks called Dynamic programming.\n",
    "\n",
    "<img src=\"imgs/improving_policy.png\">\n",
    "<img src=\"imgs/improving_policy2.png\">\n",
    "\n",
    "Dynamic programming uses various Bellman equations to iteratively compute optimal policis. DP doesn't not involve any kind of interaction with the enviroment at all, instead it relies only on the model of MDP. \n",
    "\n",
    "<img src=\"imgs/dp_bellman.png\">\n",
    "\n",
    "\n",
    "DP is very useful for understanding other RL algorithms. Most RL algorithms can be seen as an aproximation to DP solution without the model.\n",
    "\n",
    "### Iterative policy evaluation\n",
    "\n",
    "<img src=\"imgs/iterative_policy_eval1.png\">\n",
    "<img src=\"imgs/iterative_policy_eval.png\">\n",
    "\n",
    "### Policy improvement theorem\n",
    "\n",
    "<img src=\"imgs/policy_improvement_theorem.png\">\n",
    "    \n",
    "### Policy iteration\n",
    "\n",
    "<img src=\"imgs/policy_iteration_1.png\">\n",
    "<img src=\"imgs/policy_iteration_2.png\">\n",
    "\n",
    "\n",
    "### Policy iteration flexibility\n",
    "\n",
    "<img src=\"imgs/generalized_policy_iteration.png\">\n",
    "<img src=\"imgs/value_iteration.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 3. Prediction and Control with Function Approximation\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
