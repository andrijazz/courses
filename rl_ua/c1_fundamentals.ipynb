{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly readings\n",
    "\n",
    "c1.w1. Chapter 2-2.7 (Pages 25-36)\n",
    "\n",
    "c1.w2. Chapter 3.3 (pages 47-56)\n",
    "\n",
    "c1.w3. Chapter 3.5 - 3.8 (pages 58-67)\n",
    "\n",
    "c1.w4. Chapter 4.1, 4.2, 4.3, 4.4, 4.6, 4.7 (pages 73-88)\n",
    "\n",
    "c2.w1. None\n",
    "\n",
    "c2.w2. Chapter 5.0-5.5 (pp. 91-104) in the Reinforcement Learning textbook\n",
    "\n",
    "c2.w3. Chapter 6.3 (pp. 116-128)\n",
    "\n",
    "c2.w4. Chapter 6.4-6.6 (pp. 129-134)\n",
    "\n",
    "c2.w5. Chapter 8.1-8.3 (pp. 159-166) (For a summary of what we've covered in the specialization so far, read: pp. 189-191)\n",
    "\n",
    "c3.w1. Ch. 9.1 - 9.4 (pp. 197- 209) in the Reinforcement Learning textbook.\n",
    "\n",
    "c3.w2. Ch. 9.4 - 9.5.0 (pp. 204 - 210), 9.5.3 - 9.5.4 (pp 215 - 222) and 9.7 (pg 223 - 228) in the Reinforcement Learning textbook.\n",
    "\n",
    "c3.w3. Chapter 10 (pp. 243-246) & 10.3 (pp. 249-252) in the Reinforcement Learning textbook. Note: skip over section 10.2.\n",
    "\n",
    "\n",
    "\n",
    "c3.w4.\n",
    "\n",
    "c4.w1.\n",
    "\n",
    "c4.w2.\n",
    "\n",
    "c4.w3.\n",
    "\n",
    "c4.w4.\n",
    "\n",
    "c4.w5.\n",
    "\n",
    "c4.w6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1. Fundamentals of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1. The K-Armed Bandit Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.1.\n",
    "\n",
    "Greedy action will be selected in 50% of the cases. In other 50% we will have a choise between greedy and non-greedy action. That gives us $0.5 + 0.5 * 0.5 = 0.75$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2. Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: episode, dynamics of MDP, example diagram of MDP\n",
    "\n",
    "MDPs are framework that enables us to model real world problems and formalize that problem mathematicaly. In order to understand MDPs it is crucial to fully understand every term that we are about to define and because of that this note will be in the format of cheetsheet with lots of definitions. Coursera lectures, RL textbook and David Silver's course sections on MDPs are very similar and they contain same informations but David Silver's course has the best structure imo so I will base my notes on MDPs on his lecture.\n",
    "\n",
    "<img src=\"imgs/agent-env.png\">\n",
    "\n",
    "_def_. Real world problem that we want to model we call **Stochastic or Random process** in the language of statistics. We can think of a random process as a set of random states. More formaly, stochastic process is a family of random variables describing certain events.\n",
    "\n",
    "_def_. If the next state of a process depends only on present state and not on previous states we say that process has **Markov property**. This should be thought as a restictions of the states meaning that each state should capture all relevant information from history. More formaly, Markov property is defined like this:\n",
    "\n",
    "$$\n",
    "P(S_{t+1}|S_t) = P(S_{t+1}|S_1, ... ,S_{t})\n",
    "$$\n",
    "\n",
    "_def_. **State transition matrix** is a square matrix which tell us what is the probabilty of transitioning from one state to another.\n",
    "\n",
    "$$\n",
    "\\mathcal{P} = \\begin{bmatrix}\n",
    "p_{11} & .. & p_{1n}\\\\\n",
    ". &  & \\\\\n",
    ". &  & \\\\\n",
    "p_{n1} & .. & p_{nn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "_def_. **Markov process** or **Markov chain** is a process with Markov property ie. sequence of random states $S_1, S_2 ...$ with Markov property. More formaly, Markov process is a tuple $\\langle \\mathcal{S}, \\mathcal{P} \\rangle$ where $\\mathcal{S}$ is set of states and $\\mathcal{P}$ is state transition matrix.\n",
    "\n",
    "<img src=\"imgs/mp.png\">\n",
    "\n",
    "_def_. **Markov reward process** is a Markov process with value judgements - how good it is to be in a certain stated. Markov reward process is a tuple $\\langle \\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\gamma \\rangle$ where $\\mathcal{S}$ is set of states, $\\mathcal{P}$ is state transition matrix, $\\mathcal{R}$ is reward function and $\\gamma$ is a discount factor.\n",
    "\n",
    "<img src=\"imgs/mrp.png\">\n",
    "\n",
    "\n",
    "_def_. **Return** $G_t$ is a total dicounted sum of rewards you get after time $t$.\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} ... = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "_def_. The **goal** is a optimal return. The goal of reinforcement learning is to optimize return.\n",
    "\n",
    "_def_. **Value function** $v(s)$ gives the long-term value of being in a state (expectation)\n",
    "\n",
    "<img src=\"imgs/value_functions.png\">\n",
    "\n",
    "_def_. **Bellman equation for MRPs** is following equation and it basically states that value function can be decomposed into 2 parts: 1. imidiate reward $R_{t+1}$ 2. discounted value of successor state $\\gamma v(S_{t+1})$\n",
    "\n",
    "<img src=\"imgs/bellman_mrps.png\">\n",
    "\n",
    "_def_. **Backup diagram** is one-step look ahead tree which helps us visualize one step of a process.\n",
    "\n",
    "<img src=\"imgs/calc_value_function.png\">\n",
    "\n",
    "<img src=\"imgs/bellman_eq_mat.png\">\n",
    "\n",
    "Because Bellman equation is linear it is possible to be solved (calculate $v$) directly:\n",
    "\n",
    "<img src=\"imgs/solve_bellman_eq.png\">\n",
    "\n",
    "Computational complexity of this calculation is $O(n^3)$ and therefore direct solution is applicable only to small MRPs. For larger MRPs other methods are avalible: Dynamic programming, Monte-carlo evaluation and Temporal-Difference learning.\n",
    "\n",
    "_def_. **Markov decision process**\n",
    "\n",
    "<img src=\"imgs/mdp.png\">\n",
    "\n",
    "_def_. **Policy** is a mapping from states to probabilities of selecting each possible action.\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = P(A_t=a|S_t=s)\n",
    "$$\n",
    "\n",
    "<img src=\"imgs/mdp_note.png\">\n",
    "\n",
    "_def_. Previously, in MRPs, we defined a value function $v(s)$ as a long-term value of being in a state. In MDPs we are defining **state-value function**  which tells us how good is to be in the state following the policy $\\pi$.State-value function is defined as expected return starting from $s$ following policy $\\pi$.\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = E_{\\pi}(G_t|S_t=s)\n",
    "$$\n",
    "\n",
    "_def_. **Action-value function** tells us how good is to take certain action from a peticular state while following the policy $\\pi$. We defined action-value function $q_{\\pi}(s, a)$ as a expected return when starting from state $s$, taking the action $a$ while following policy $\\pi$:\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = E_{\\pi}(G_t|S_t=s, A_t=a)\n",
    "$$\n",
    "\n",
    "\n",
    "We can now also define how Bellman equation looks like in MDPs.\n",
    "\n",
    "_def_. **Bellman expectation equation for state-value function**:\n",
    "\n",
    "<img src=\"imgs/bellman_eq_sv.png\">\n",
    "\n",
    "<!--- \n",
    "$$\n",
    "\\mathsf{v}_{\\pi}(s) = \\mathbb{E}_{\\pi}(R_{t+1} + \\gamma\\mathsf{v}_{\\pi}(S_{t+1}) | S_t = s) = \\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s, a)\n",
    "$$\n",
    " -->\n",
    "_def_. **Bellman expectation equation for action-value function**:\n",
    "<img src=\"imgs/bellman_eq_av.png\">\n",
    "<!-- \n",
    "$$\n",
    "q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}(R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a) = R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{ss'}^{a}\\mathsf{v}_{\\pi}(s')\n",
    "$$\n",
    " -->\n",
    " \n",
    "Therefore,\n",
    "<img src=\"imgs/e_bellman_eq1.png\">\n",
    "<img src=\"imgs/e_bellman_eq2.png\">\n",
    "<img src=\"imgs/e_bellman_eq3.png\">\n",
    "<img src=\"imgs/e_bellman_eq4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3. Optimality\n",
    "\n",
    "_def_. **Optimal value functions**\n",
    "<img src=\"imgs/optimal_value_functions.png\">\n",
    "\n",
    "_def_. **Optimal policy**\n",
    "<img src=\"imgs/optimal_policy.png\">\n",
    "\n",
    "_def_. **Bellman optimality equation**\n",
    "<img src=\"imgs/o_bellman_eq1.png\">\n",
    "<img src=\"imgs/o_bellman_eq2.png\">\n",
    "<img src=\"imgs/o_bellman_eq3.png\">\n",
    "<img src=\"imgs/o_bellman_eq4.png\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4. Dynamic Programming\n",
    "\n",
    "**Dynamic programming** is the method for solving complex problems by 1. breaking them down into subproblems -> 2. solve the subproblems -> 3. combine solutions to subproblems\n",
    "\n",
    "Dynamic Programming is a very general solution method for problems which have two properties:\n",
    "1. Optimal substructure\n",
    "    * Principle of optimality applies\n",
    "    * Optimal solution can be decomposed into subproblems\n",
    "2. Overlapping subproblems\n",
    "    * Subproblems recur many times\n",
    "    * Solutions can be cached and reused\n",
    "    \n",
    "MDPs satisfy both properties - Bellman equation gives recursive decomposition and value function stores and reuses solutions (value function is the *cache*).\n",
    "\n",
    "_def_. **Policy evaluation** is the task of determening state-value function for a specific policy\n",
    "\n",
    "<img src=\"imgs/policy_eval.png\">\n",
    "\n",
    "_def_. **Control** is the task of finding the policy to obtain as much as reward as possible - finding the policy which maximises the value function.\n",
    "\n",
    "Control is a ultimate goal of Reinforcement learning but the task of policy evaluation is the necessary first step. This week we will be looking at collection of algorithms used for solving policy evaluation and control tasks called Dynamic programming.\n",
    "\n",
    "<img src=\"imgs/improving_policy.png\">\n",
    "<img src=\"imgs/improving_policy2.png\">\n",
    "\n",
    "Dynamic programming uses various Bellman equations to iteratively compute optimal policis. DP doesn't not involve any kind of interaction with the enviroment at all, instead it relies only on the model of MDP. \n",
    "\n",
    "<img src=\"imgs/dp_bellman.png\">\n",
    "\n",
    "\n",
    "DP is very useful for understanding other RL algorithms. Most RL algorithms can be seen as an aproximation to DP solution without the model.\n",
    "\n",
    "### Iterative policy evaluation\n",
    "\n",
    "<img src=\"imgs/iterative_policy_eval1.png\">\n",
    "<img src=\"imgs/iterative_policy_eval.png\">\n",
    "\n",
    "### Policy improvement theorem\n",
    "\n",
    "<img src=\"imgs/policy_improvement_theorem.png\">\n",
    "    \n",
    "### Policy iteration\n",
    "\n",
    "<img src=\"imgs/policy_iteration_1.png\">\n",
    "<img src=\"imgs/policy_iteration_2.png\">\n",
    "\n",
    "\n",
    "### Policy iteration flexibility\n",
    "\n",
    "<img src=\"imgs/generalized_policy_iteration.png\">\n",
    "<img src=\"imgs/value_iteration.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
