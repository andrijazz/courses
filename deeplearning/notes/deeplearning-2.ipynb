{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Dev / Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of hyperparameters:\n",
    "* Number of layers\n",
    "* Number of hidden units in each layer\n",
    "* Learning rates\n",
    "* Regularization parameters\n",
    "* Activation functions\n",
    "\n",
    "In order to figure out what are the most suitable hyperparameter values for your neural net it is advised that you constantly experiment with different values and iterate on repeating the experiments.\n",
    "\n",
    "Previous era of machine learning was characterized by the fact that we had much smaller amounts of data. \n",
    "* 70 / 30 train / test split or 60 / 20 / 20 train / dev / test split was ok when you have up to 10000 examples in your dataset, but when you have 1m examples 98 / 1 / 1 is also good choise\n",
    "* Make sure that dev and test set come from the same distribution\n",
    "* Not having the test set might be ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "\n",
    "<img src=\"imgs/biasvariance.png\">\n",
    "\n",
    "It is possible that classifer has both high bias and high variance and that would look something like this:\n",
    "\n",
    "<img src=\"imgs/highbias&highvariance.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic recipe for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Try bigger network\n",
      "* Train longer\n",
      "* Search different nn architecture\n",
      "* Find more data or create more features\n",
      "* Regularization\n",
      "* Search different nn architecture\n"
     ]
    }
   ],
   "source": [
    "high_bias = True # measure error on train set \n",
    "high_variance = True # measure error on dev / test set\n",
    "\n",
    "if high_bias:\n",
    "    print(\"* Try bigger network\")\n",
    "    print(\"* Train longer\")\n",
    "    print(\"* Search different nn architecture\")\n",
    "\n",
    "if high_variance:\n",
    "    print(\"* Find more data or create more features\")\n",
    "    print(\"* Regularization\")\n",
    "    print(\"* Search different nn architecture\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Let introduce regularization on an example of Logistic Regression.\n",
    "\n",
    "L2 regularization:\n",
    "\\begin{align}\n",
    "J(W, b) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{2m}\\sum_{j = 1}^{n_x} {\\lVert w_j \\rVert}^2\n",
    "\\end{align}\n",
    "\n",
    "L1 regularization:\n",
    "\\begin{align}\n",
    "J(W, b) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{m}\\sum_{j = 1}^{n_x} {\\lVert w_j \\rVert}\n",
    "\\end{align}\n",
    "\n",
    "In general, L2 regularization is used much much more often. L1 regularization is used when we want to make $W$ sparse.\n",
    "\n",
    "In the case of Neural Networks L2 regularization would look like this:\n",
    "\\begin{align}\n",
    "J(W_1, b_1, W_2, b_2 ... W_L, b_L) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{2m}\\sum_{l = 1}^{L} {\\lVert W_l \\rVert}^2 = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{2m}\\sum_{i = 1}^{n_{l - 1}}\\sum_{j = 1}^{n_l} {\\lVert w_{ij} \\rVert}^2\n",
    "\\end{align}\n",
    "\n",
    "because $W$'s are always $n_{l-1}$ x $n_l$ dimension.\n",
    "\n",
    "Note that L2 norm is also called Frobenius norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Regularization Reduces Overfitting\n",
    "\n",
    "Couple of good points .. watch it again\n",
    "https://www.youtube.com/watch?v=NyG-7nRpsW8&index=5&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a efficient way of regularizing neural network paramenters.\n",
    "\n",
    "Let say this is our neural network and what we are going to do is that for each layer we are going to set the probability of keeping (eliminating) each node in that layer. Then, when we do training, after one training example we decide to remove certain nodes (by removing all in-going and out-going links of that node) so we end up with much smaller network. After we do backpropagation new training instance comes along and then we choose different nodes to be eliminated and so on.\n",
    "\n",
    "<img src=\"imgs/dropout1.png\">\n",
    "<img src=\"imgs/dropout2.png\">\n",
    "\n",
    "The parameter that represents probability of keeping each node is called `keep-prob`. There are several ways to implement dropout regularization but the most common one is `Inverted dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "[[ 1.18609307]\n",
      " [ 1.03969519]\n",
      " [ 1.14077036]\n",
      " [ 0.03622744]]\n"
     ]
    }
   ],
   "source": [
    "# inverted dropout for layer 3, keepProb = 0.8\n",
    "import numpy as np\n",
    "keepProb = 0.8\n",
    "a3 = np.random.rand(4, 1)\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keepProb\n",
    "a3 = np.multiply(a3, d3) # element-wise multiplication\n",
    "a3 /= 0.8\n",
    "# z4 = w4 * a3 + b4\n",
    "print(d3)\n",
    "print(a3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't use dropout when making a prediction. More info and the math behind dropout can be found it Hintons paper http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout works really well in computer vision problems and it is used almost as a default because you almost never have enough data and model is usually overfitting. In other areas of ai is not used that frequently. One downside of dropout is that cost function J may not be monotonically decreasing. Because of that prefered way of debbuging the code is to plot cost function J without dropout (keep-prob is set to 1) and if everything is ok, switch on dropout by setting keep-prob to `0.8` or some other value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping\n",
    "Data augmentation\n",
    "Orthogonalization\n",
    "\n",
    "<img src=\"imgs/earlystopping.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Inputs\n",
    "\n",
    "Let $X = [x_1, x_2, ..., x_n] \\in R^{m \\times n}$ be our data, where $x_1, ...x_n$ are feature vectors. You can imagine that our data looks like this for $n = 2$:\n",
    "\n",
    "<img src=\"imgs/normalization1.png\">\n",
    "\n",
    "We first subtract the mean of each feature from $X - \\mu(X) = [x_1 - \\frac{1}{m}\\sum_{i = 1}^{m}x_{i, 1}, .. , x_n - \\frac{1}{m}\\sum_{i = 1}^{m}x_{i, n}]$ and by doing that we are centering our data:\n",
    "\n",
    "<img src=\"imgs/normalization2.png\">\n",
    "\n",
    "And then, devide by standard deviation $\\frac{X - \\mu(X)}{\\sigma^2(X)} = [\\frac{x_1 - \\frac{1}{m}\\sum_{i = 1}^{m}x_{i, 1}}{\\frac{1}{m}\\sum_{i = 1}^{m}x^2_{i, 1}}, .. , \\frac{x_n - \\frac{1}{m}\\sum_{i = 1}^{m}x_{i, n}}{\\frac{1}{m}\\sum_{i = 1}^{m}x^2_{i, n}}]$ where now variance of all features is equal to 1:\n",
    "\n",
    "<img src=\"imgs/normalization3.png\">\n",
    "\n",
    "__Variance__ measures how far a set of (random) numbers are spread out from their average value (mean). And __standard deviation__ is squared variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing and exploading gradients\n",
    "\n",
    "When training very deep neural network we can encounter problem where gradients are too small (vanishing) or too big (exploading) which makes the training difficult. Problem of vanishing / exploading gradients can be partialy solved by carefull weight initialization depending on what activation function are you using:\n",
    "\n",
    "## Weight Initialization in a Deep Network\n",
    "\n",
    "1) If you are using $ReLU$ weights should be initialized to (he-at-al initialization https://arxiv.org/abs/1502.01852) and what it does is basically setting the variance of $w$ to $\\frac{2}{n_l}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "layer_size = [5, 4]\n",
    "l = 1\n",
    "w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) If you are using $Tanh$ weights should be initialized to (Xavier initialization http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "layer_size = [5, 4]\n",
    "l = 1\n",
    "w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or this (Bengio initialization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "layer_size = [5, 4]\n",
    "l = 1\n",
    "w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l] + layer_size[l-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical aproximation of gradients\n",
    "## Gradient checking\n",
    "## Gradient checking implementation notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization algorithms - mini batch gradient descent (week 2 beginning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning process\n",
    "\n",
    "* Don't use grid search for hyperparameter search. Instead, use random sampling over the range of hyperparameters.\n",
    "* Consider implementing zoom in of the region that gives the best results and then repeat the process with ranges set to values that are within region that gave the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice basic tutorial ... very basic though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
