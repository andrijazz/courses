{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Dev / Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of hyperparameters:\n",
    "* Number of layers\n",
    "* Number of hidden units in each layer\n",
    "* Learning rates\n",
    "* Regularization parameters\n",
    "* Activation functions\n",
    "\n",
    "In order to figure out what are the most suitable hyperparameter values for your neural net it is advised that you constantly experiment with different values and iterate on repeating the experiments.\n",
    "\n",
    "Previous era of machine learning was characterized by the fact that we had much smaller amounts of data. \n",
    "* 70 / 30 train / test split or 60 / 20 / 20 train / dev / test split was ok when you have up to 10000 examples in your dataset, but when you have 1m examples 98 / 1 / 1 is also good choise\n",
    "* Make sure that dev and test set come from the same distribution\n",
    "* Not having the test set might be ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "\n",
    "<img src=\"imgs/biasvariance.png\">\n",
    "\n",
    "It is possible that classifer has both high bias and high variance and that would look something like this:\n",
    "\n",
    "<img src=\"imgs/highbias&highvariance.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic recipe for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Try bigger network\n",
      "* Train longer\n",
      "* Search different nn architecture\n",
      "* Find more data or create more features\n",
      "* Regularization\n",
      "* Search different nn architecture\n"
     ]
    }
   ],
   "source": [
    "high_bias = True # measure error on train set \n",
    "high_variance = True # measure error on dev / test set\n",
    "\n",
    "if high_bias:\n",
    "    print(\"* Try bigger network\")\n",
    "    print(\"* Train longer\")\n",
    "    print(\"* Search different nn architecture\")\n",
    "\n",
    "if high_variance:\n",
    "    print(\"* Find more data or create more features\")\n",
    "    print(\"* Regularization\")\n",
    "    print(\"* Search different nn architecture\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Let introduce regularization on an example of Logistic Regression.\n",
    "\n",
    "L2 regularization:\n",
    "\\begin{align}\n",
    "J(W, b) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{2m}\\sum_{j = 1}^{n_x} {\\lVert w_j \\rVert}^2\n",
    "\\end{align}\n",
    "\n",
    "L1 regularization:\n",
    "\\begin{align}\n",
    "J(W, b) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{m}\\sum_{j = 1}^{n_x} {\\lVert w_j \\rVert}\n",
    "\\end{align}\n",
    "\n",
    "In general, L2 regularization is used much much more often. L1 regularization is used when we want to make $W$ sparse.\n",
    "\n",
    "In the case of Neural Networks L2 regularization would look like this:\n",
    "\\begin{align}\n",
    "J(W_1, b_1, W_2, b_2 ... W_L, b_L) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{2m}\\sum_{l = 1}^{L} {\\lVert W_l \\rVert}^2 = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{2m}\\sum_{i = 1}^{n_{l - 1}}\\sum_{j = 1}^{n_l} {\\lVert w_{ij} \\rVert}^2\n",
    "\\end{align}\n",
    "\n",
    "because $W$'s are always $n_{l-1}$ x $n_l$ dimension.\n",
    "\n",
    "Note that L2 norm is also called Frobenius norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Regularization Reduces Overfitting\n",
    "\n",
    "Couple of good points .. watch it again\n",
    "https://www.youtube.com/watch?v=NyG-7nRpsW8&index=5&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout regularization\n",
    "\n",
    "## Understanding dropout\n",
    "\n",
    "## Other regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Inputs\n",
    "\n",
    "Let $X = [x_1, x_2, ..., x_n] \\in R^{m \\times n}$ be our data, where $x_1, ...x_n$ are feature vectors. You can imagine that our data looks like this for $n = 2$:\n",
    "\n",
    "<img src=\"imgs/normalization1.png\">\n",
    "\n",
    "We first subtract the mean of each feature from $X - \\mu(X) = [x_1 - \\frac{1}{m}\\sum_{i = 1}^{m}x_{i, 1}, .. , x_n - \\frac{1}{m}\\sum_{i = 1}^{m}x_{i, n}]$ and by doing that we are centering our data:\n",
    "\n",
    "<img src=\"imgs/normalization2.png\">\n",
    "\n",
    "And then, devide by standard deviation $\\frac{X - \\mu(X)}{\\sigma^2(X)} = [\\frac{x_1 - \\frac{1}{m}\\sum_{i = 1}^{m}x_{i, 1}}{\\frac{1}{m}\\sum_{i = 1}^{m}x^2_{i, 1}}, .. , \\frac{x_n - \\frac{1}{m}\\sum_{i = 1}^{m}x_{i, n}}{\\frac{1}{m}\\sum_{i = 1}^{m}x^2_{i, n}}]$ where now variance of all features is equal to 1:\n",
    "\n",
    "<img src=\"imgs/normalization3.png\">\n",
    "\n",
    "__Variance__ measures how far a set of (random) numbers are spread out from their average value (mean). And __standard deviation__ is squared variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing and exploading gradients\n",
    "\n",
    "When training very deep neural network we can encounter problem where gradients are too small (vanishing) or too big (exploading) which makes the training difficult. Problem of vanishing / exploading gradients can be partialy solved by carefull weight initialization depending on what activation function are you using:\n",
    "\n",
    "1) If you are using $ReLU$ weights should be initialized to (he-at-al initialization https://arxiv.org/abs/1502.01852) and what it does is basically setting the variance of $w$ to $\\frac{2}{n_l}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "layer_size = [5, 4]\n",
    "l = 1\n",
    "w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) If you are using $Tanh$ weights should be initialized to (Xavier initialization http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "layer_size = [5, 4]\n",
    "l = 1\n",
    "w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or this (Bengio initialization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "layer_size = [5, 4]\n",
    "l = 1\n",
    "w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l] + layer_size[l-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical aproximation of gradients\n",
    "## Gradient checking\n",
    "## Gradient checking implementation notes\n",
    "\n",
    "## Complete week 2 was skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning process\n",
    "\n",
    "* Don't use grid search for hyperparameter search. Instead, use random sampling over the range of hyperparameters.\n",
    "* Consider implementing zoom in of the region that gives the best results and then repeat the process with ranges set to values that are within region that gave the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
