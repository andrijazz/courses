{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Dev / Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of hyperparameters:\n",
    "* Number of layers\n",
    "* Number of hidden units in each layer\n",
    "* Learning rates\n",
    "* Regularization parameters\n",
    "* Activation functions\n",
    "\n",
    "In order to figure out what are the most suitable hyperparameter values for your neural net it is advised that you constantly experiment with different values and iterate on repeating the experiments.\n",
    "\n",
    "Previous era of machine learning was characterized by the fact that we had much smaller amounts of data. \n",
    "* 70 / 30 train / test split or 60 / 20 / 20 train / dev / test split was ok when you have up to 10000 examples in your dataset, but when you have 1m examples 98 / 1 / 1 is also good choise\n",
    "* Make sure that dev and test set come from the same distribution\n",
    "* Not having the test set might be ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "\n",
    "<img src=\"imgs/biasvariance.png\">\n",
    "\n",
    "It is possible that classifer has both high bias and high variance and that would look something like this:\n",
    "\n",
    "<img src=\"imgs/highbias&highvariance.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic recipe for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Try bigger network\n",
      "* Train longer\n",
      "* Search different nn architecture\n",
      "* Find more data or create more features\n",
      "* Regularization\n",
      "* Search different nn architecture\n"
     ]
    }
   ],
   "source": [
    "high_bias = True # measure error on train set \n",
    "high_variance = True # measure error on dev / test set\n",
    "\n",
    "if high_bias:\n",
    "    print(\"* Try bigger network\")\n",
    "    print(\"* Train longer\")\n",
    "    print(\"* Search different nn architecture\")\n",
    "\n",
    "if high_variance:\n",
    "    print(\"* Find more data or create more features\")\n",
    "    print(\"* Regularization\")\n",
    "    print(\"* Search different nn architecture\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Let introduce regularization on an example of Logistic Regression.\n",
    "\n",
    "L2 regularization:\n",
    "\\begin{align}\n",
    "J(W, b) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{2m}\\sum_{j = 1}^{n_x} {\\lVert w_j \\rVert}^2\n",
    "\\end{align}\n",
    "\n",
    "L1 regularization:\n",
    "\\begin{align}\n",
    "J(W, b) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{m}\\sum_{j = 1}^{n_x} {\\lVert w_j \\rVert}\n",
    "\\end{align}\n",
    "\n",
    "In general, L2 regularization is used much much more often. L1 regularization is used when we want to make $W$ sparse.\n",
    "\n",
    "In the case of Neural Networks L2 regularization would look like this:\n",
    "\\begin{align}\n",
    "J(W_1, b_1, W_2, b_2 ... W_L, b_L) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{2m}\\sum_{l = 1}^{L} {\\lVert W_l \\rVert}^2 = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{2m}\\sum_{i = 1}^{n_{l - 1}}\\sum_{j = 1}^{n_l} {\\lVert w_{ij} \\rVert}^2\n",
    "\\end{align}\n",
    "\n",
    "because $W$'s are always $n_{l-1}$ x $n_l$ dimension.\n",
    "\n",
    "Note that L2 norm is also called Frobenius norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Regularization Reduces Overfitting\n",
    "\n",
    "Couple of good points .. watch it again\n",
    "https://www.youtube.com/watch?v=NyG-7nRpsW8&index=5&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout regularization\n",
    "\n",
    "## Understanding dropout\n",
    "\n",
    "## Other regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Inputs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
