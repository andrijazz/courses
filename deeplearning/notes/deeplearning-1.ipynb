{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1. Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning\n",
    "\n",
    "We are introducing $ReLU$ (Rectified Linear Unit) function which is used as a activation function. \n",
    "\\begin{align}\n",
    "f(x) =\n",
    "\t\\begin{cases}\n",
    "\t\tx   & \\quad x \\geq 0\\\\\n",
    "\t\t0\t& \\quad x < 0\n",
    "\t\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "One of the major breakthroughs in NN computations was introducing $ReLU$ function instead of $Sigmoid$ function $f(x) = \\frac{1}{1 + e ^{-x}}$. The advantage of $ReLU$ is that has value $0$ when $x < 0$, where $Sigmoid$ function has values that are close to $0$ and thus making the computation harder.\n",
    "Additionally, gradient of $ReLU$ function is equal to $1$ for all positive $x$'s and thus making the gradient descent runs much faster.\n",
    "\n",
    "Especially interesting in Week 1 of the course was interview with Geoffrey Hinton where a lot of interesting ideas were mentioned so it would be really beneficial to watch this video again once I gain more knowledge about different models like Boltzman Machines etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d500a92e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "x = np.array([-2, -1, 0, 1, 2, 3])\n",
    "y = relu(x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Rectified Linear Unit')\n",
    "# plt.grid(True)\n",
    "plt.savefig(\"relu.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../plots/relu.png\" style=\"width:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image representation\n",
    "\n",
    "Image is represented with the 3-dimensional matrix where dimensions represent values of Red Green and Blue respectively. 3-dimensional matrix is usually squeezed into 1-dimensional vector and this is what we are considering to be our $x$ in terms of image classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification and Neural Network Notation\n",
    "In this section Andrew Ng defines binary classification problem, introduces Logistic Regression and $Sigmoid$ function but more importantly it introduces neural network notation that will be used throughout the course. He emphasizes that neural network notation will be different from the one used in his first Machine Learning course in a way that bias will be kept separately of parameters vector because it is more natural notation when implementing neural net.\n",
    "\n",
    "For detail notation take a look at the file $\\textit{neuralnetworknotation.pdf}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression cost function\n",
    "We define a \\textbf{loss function} as a measurement of how well we are doing on a single training example and \\textbf{cost function} of how well we are doing on the entire training set.\n",
    "\n",
    "Loss function for Logistic regression is:\n",
    "\n",
    "\\begin{align}\n",
    "L(y, \\hat{y}) = -(y\\log{\\hat{y}} + (1 - y)\\log{(1 - \\hat{y})})\n",
    "\\end{align}\n",
    "\n",
    "where $\\hat{y} = \\sigma(w^Tx + b)$ and $\\sigma$ is $Sigmoid$ function (in literature when we write $\\log$ it usually stands for logarithm with the base of $e$). The questions that pops up is why we simply can't use $L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2$ as a loss function? The answer is that optimization problem that we will encounter will become non-convex and thus gradient descent may converge to local optimum instead of global optimum.\n",
    "\n",
    "Cost function for Logistic regression is:\n",
    "\\begin{align}\n",
    "J(W, b) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) = -\\frac{1}{m}\\sum_{i = 1}^{m}(y^{(i)}\\log{\\hat{y}^{(i)}} + (1 - y^{(i)})\\log{(1 - \\hat{y}^{(i)})})\n",
    "\\end{align}\n",
    "\n",
    "People usually don't do random initialization for Logistic Regression because cost function is convex and thus the usual way is to simply initialize all the parameters to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting in Python\n",
    "\n",
    "General principle for the python broadcasting is that if you have $(m, n)$ matrix and $(m, 1)$ matrix you would be able to apply any of operations $+, -, *, /$ because second matrix will be automatically extended up to $(m, n)$ by repeating the column $n$ times. Similarly, if you have $(m, n)$ matrix and $(1, n)$ matrix you would be able to apply arithmetic operations because second matrix will be automatically extended up to $(m, n)$ by repeating the raw $m$ times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Avoid using rank 1 arrays, they are not column vectors nor row vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "[ 0.50396686  0.68641748  0.99727146  0.78588024  0.1727076 ]\n",
      "(1, 5)\n",
      "[[ 0.50396686  0.68641748  0.99727146  0.78588024  0.1727076 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.rand(5)\n",
    "\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "a = a.reshape(1, 5)\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Use assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(a.shape == (1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Logistic Regression cost function\n",
    "\n",
    "Our task is to model the probability $p(y|x)$, so we need to come up with the function, lets name it $\\hat{y}$, that will satisfy the requirements: when $y = 1$, $p(y|x) = \\hat{y}$ and when $y = 0$, $p(y|x) = 1 - \\hat{y}$. These 2 cases described $p(y|x)$ so we are now going to try to write $p(y|x)$ in a single equation: \n",
    "\n",
    "\\begin{align}\n",
    "p(y|x) = \\hat{y}^y (1 - \\hat{y})^{1 - y}\n",
    "\\end{align}\n",
    "\n",
    "Since $\\log$ is strictly monotonically increasing function, instead of modeling $p(y|x)$ we can model $\\log(p(y|x)$ and that would give us the same results. Introducing the log function will give us something like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\log{p(y|x)} = y\\log{\\hat{y}} + (1 - y)\\log{(1 - \\hat{y})}\n",
    "\\end{align}\n",
    "\n",
    "In machine learning we usually want to minimize the loss and that means maximizing the $p(y|x)$ and thus we define $L(y, \\hat{y}) = -p(y|x)$. We can now define $\\hat{y}$ as a simple polinomial function and add sigmoid to make it satisify probability requirements to be between $0$ and $1$. Thus $\\hat{y} = \\sigma(\\theta^Tx + b)$. Also, when caluclating gradients sigmoid function has a convinient property $\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$ so that is something that we should keep in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $\\frac{x}{\\| x\\|}$ (dividing each row vector of x by its norm).\n",
    "For example, if $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}$$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "Softmax or normalized exponential function is a generalization of Sigmoid function that maps vectors from $\\mathbb{R}^n$ into $n$ dimensional vector of real numbers from $[0, 1]$ range, that adds up to $1$.\n",
    "\n",
    "$$\n",
    "softmax: \\mathbb{R}^n \\rightarrow [0, 1]^n\n",
    "$$\n",
    "\n",
    "$$\n",
    "softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2  &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming assignement - cats classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b ∗ c ∗ d, a) is to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 10, 10, 3)\n",
      "(300, 200)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(200, 10, 10 , 3)\n",
    "print(X.shape)\n",
    "X_flatten = X.reshape(X.shape[0], -1).T\n",
    "print(X_flatten.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel). Lets standardize our data.\n",
    "\n",
    "<!-- During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !--> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
