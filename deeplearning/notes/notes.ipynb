{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning specialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1. Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips on numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Avoid using rank 1 arrays, they are not column vectors nor row vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "[ 0.50396686  0.68641748  0.99727146  0.78588024  0.1727076 ]\n",
      "(1, 5)\n",
      "[[ 0.50396686  0.68641748  0.99727146  0.78588024  0.1727076 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.rand(5)\n",
    "\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "a = a.reshape(1, 5)\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Use assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(a.shape == (1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of Logistic Regression cost function\n",
    "\n",
    "Our task is to model the probability $p(y|x)$, so we need to come up with the function, lets name it $\\hat{y}$, that will satisfy the requirements: when $y = 1$, $p(y|x) = \\hat{y}$ and when $y = 0$, $p(y|x) = 1 - \\hat{y}$. These 2 cases described $p(y|x)$ so we are now going to try to write $p(y|x)$ in a single equation: \n",
    "\n",
    "\\begin{align}\n",
    "p(y|x) = \\hat{y}^y (1 - \\hat{y})^{1 - y}\n",
    "\\end{align}\n",
    "\n",
    "Since $\\log$ is strictly monotonically increasing function, instead of modeling $p(y|x)$ we can model $\\log(p(y|x)$ and that would give us the same results. Introducing the log function will give us something like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\log{p(y|x)} = y\\log{\\hat{y}} + (1 - y)\\log{(1 - \\hat{y})}\n",
    "\\end{align}\n",
    "\n",
    "In machine learning we usually want to minimize the loss and that means maximizing the $p(y|x)$ and thus we define $L(y, \\hat{y}) = -p(y|x)$. We can now define $\\hat{y}$ as a simple polinomial function and add sigmoid to make it satisify probability requirements to be between $0$ and $1$. Thus $\\hat{y} = \\sigma(\\theta^Tx + b)$. Also, when caluclating gradients sigmoid function has a convinient property $\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$ so that is something that we should keep in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
