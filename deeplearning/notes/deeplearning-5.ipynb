{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 5. Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1 - Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some applications that can be solved using sequence models are: speech recognition (input audio clip, output sequence of words), music generation (no input, output notes), sentiment classification (input sentance, output rating), DNA sequence analysis (input string of AGCT ... output sequence of matching protein), machine translation, video activity recognition, name entity recognition etc.\n",
    "\n",
    "Its also intersting to note that RNN are first introduced in 1986. (https://en.wikipedia.org/wiki/Recurrent_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name entity recognition example:\n",
    "<img src=\"imgs/name_entity_recognition.png\">\n",
    "\n",
    "Notation:\n",
    "\n",
    "$x^{(i)}$ - $i$-th training example \n",
    "\n",
    "$x^{(i)<t>}$ - $t$-th element in the sequence of $i$-th training example\n",
    "\n",
    "$T_{x}^{(i)}$ - length of the $i$-th training example sequence\n",
    "\n",
    "$y^{(i)<t>}$ - $t$-th element of the output sequence of $i$-th training example\n",
    "\n",
    "$T_{y}^{(i)}$ - length of the output sequence of $i$-th training example\n",
    "\n",
    "<img src=\"imgs/representing_words.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "Why not use standard nn for solving for example name entity recognition based on a sentance?\n",
    "1. Input sentances won't always have same length\n",
    "2. Even if we find a way around that (for example define maximum length for an input layer) this type of model doesnt share learned features across different positions of the text \n",
    "\n",
    "The better way to apporach this problem is with reccurent neural nets.\n",
    "<img src=\"imgs/rnn.png\">\n",
    "\n",
    "We define the notation $w_{ax}$ in such that left subscript letter suggest what is being computed - in this example it is activation $a$ and right subscript letter suggest what is the multiplier of the parameters - int this example it is some input vector $x$ \n",
    "\n",
    "<img src=\"imgs/forwardproprnn.png\">\n",
    "\n",
    "In order to simplify notation a bit we would stack $w_{aa}$ and $w_{ax}$ togeather into a $[w_{aa}; w_{ax}] = w_a$. This also implies that we have to stack $a_{t-1}$ and $x_t$ into a single column vector $[a_{t-1}, x_t]$ as well.\n",
    "<img src=\"imgs/simplified_rnn_notation.png\">\n",
    "\n",
    "What we got at the end is following notation:\n",
    "\\begin{align}\n",
    "a_t = g(w_a [a_{t-1}, x] + b_a) \\\\\n",
    "y_t = g(w_y a_t + b_y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation through time\n",
    "\n",
    "After forward propagation is finished (from left to right) we then have to go back and do backprop steps for inputs that happen before and thats why rnn back prop algorithm is called back propagation through time.\n",
    "<img src=\"imgs/back_prop_through_time.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of RNN\n",
    "\n",
    "Reference: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "There are several types of RNN architectures:\n",
    "* one to one - standard nn\n",
    "* one to many - music generation (for a given small set of notes or genre for example nn generates music piece)\n",
    "* many to one - sentiment classification (for example if the movie review is positive or negative)\n",
    "* many to many where $T_x = T_y$ - for example name entity recognition\n",
    "* many to many where $T_x \\neq T_y$ - machine translation\n",
    "\n",
    "<img src=\"imgs/rnn_types3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model and sequence generation\n",
    "\n",
    "If you say \"The apple and pair salad\" vs \"The apple and pear salad\" good speech recognition system should be able to tell the difference and figure out that second sentance has much more sense. The way speech recognition system know what sentance to use is by using a language model ie it assignes each sentance a probability.\n",
    "\n",
    "Language model is a RNN which produces prediction what would be the next word based on a given word. Building language model steps assumes having large corpus and tokenazing text (EOS, UNK additional tokens).\n",
    "\n",
    "<img src=\"imgs/language_model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling novel sequences\n",
    "\n",
    "<img src=\"imgs/sampling_from_lm.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradients with RNNs\n",
    "\n",
    "As with deep neural nets there is a chance that gradients increase (explode) or decrease (vanish) exponentialy. To address exploding gradient problem what usually works well is gradient clipping techinque. To address vanishing gradient problem we are introducing Gated Reccurent Unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit GRU (watch again)\n",
    "\n",
    "This is how rnn unit looks like:\n",
    "<img src=\"imgs/rnn_unit.png\">\n",
    "\n",
    "Now lets explain gated recurrent unit [1][2].\n",
    "\n",
    "**The cat, which already ate ... was full**\n",
    "\n",
    "In this example we want for our network to memorize that subject in the sentance (cat) was singular and based on that to generate text ... _was full_ vs _were full_.\n",
    "\n",
    "<img src=\"imgs/gru_unit.png\">\n",
    "\n",
    "Full GRU\n",
    "\n",
    "<img src=\"imgs/full_gru_unit.png\">\n",
    "\n",
    "References:\n",
    "1. [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n",
    "2. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory (LSTM)\n",
    "\n",
    "<img src=\"imgs/lstm.png\">\n",
    "\n",
    "Both, GRU and LSTM has their advandages and disadvantages:\n",
    "* GRU is much simpler model so its easier to build much bigger network\n",
    "* LSTM is more powerful and flexible\n",
    "\n",
    "References:\n",
    "1. https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN\n",
    "\n",
    "<img src=\"imgs/bidirectional_rnn_1.png\">\n",
    "<img src=\"imgs/bidirectional_rnn_2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep RNNs\n",
    "\n",
    "<img src=\"imgs/deep_rnn.png\">\n",
    "\n",
    "Comparing with deep neural nets that we saw in previous lectures having 3 hidden layers for RNN is already huge because of the amount of computations that is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 - Natural Language Processing & Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word representation\n",
    "\n",
    "Explanation of what is word embedding:\n",
    "<img src=\"imgs/word_embeddings.png\">\n",
    "\n",
    "There are several commonly used algorithms for visualising word embeddings (projecting the word vectors on 2D plane) and one of them is [t-sne](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word embeddings\n",
    "\n",
    "<img src=\"imgs/transfer_learning_word_embeddings.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of word embeddings\n",
    "\n",
    "Paper [1] presented idea of using embeddings differences to figure out analogies between the words. Important thing to note is that after applying t-sne to embeddings (after projecting them on 2D plane) embeddings lose the analogies info\n",
    "<img src=\"imgs/analogies.png\">\n",
    "<img src=\"imgs/cosine_similarity.png\">\n",
    "\n",
    "References:\n",
    "1. [Linguistic Regularities in Continuous Space Word Representations (2016)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix\n",
    "\n",
    "Embedding matrix is matrix in which columns are word vectors. Size of embedding matrix is `embedding_dim` x `vocabulary_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning word embeddings\n",
    "\n",
    "We are solving the problem of language modeling and for a given a sequence of words we want to predict next word. This can be done like this:\n",
    "1. randomly initialize matrix of embeddings $E$\n",
    "2. stack $k$ word embeddings into a matrix and feed them into the neural network (in this example $k = 6$)\n",
    "3. neural network will have softmax at the end which would give probabilities of what is the next word in the sequence for each word in the vocabulary ([reference paper](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf))\n",
    "\n",
    "<img src=\"imgs/natural_language_model.png\">\n",
    "\n",
    "So, what we saw on the last slide was that the job of the algorithm was to predict some word `juice`, which we are going to call the **target** words, and it was given some **context** which was the last four words. And so, if your goal is to learn a embedding of researchers have experimented with many different types of context. \n",
    "\n",
    "<img src=\"imgs/context_target_pairs.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec skip gram model\n",
    "\n",
    "Word2vec is almost the same algorithm as the previous one, only the context is choosen differently. In word2vec model contex is a single word left of the target word. Word2vec model is described in [this](https://arxiv.org/abs/1301.3781) paper. Very important thing to note is that the goal of skip gram word2vec model is not to perform well on the task (given a single context word predict the next word) but to efficiently learn word representations.\n",
    "\n",
    "Note: Usual loss for softmax unit is negative log-likelyhood $L(y, \\hat{y}) = -\\sum_{j=1}^{10000}y_ilog\\hat{y_i}$\n",
    "\n",
    "<img src=\"imgs/skip_grams.png\">\n",
    "<img src=\"imgs/word2vec.png\">\n",
    "<img src=\"imgs/hiararchical_softmax.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling\n",
    "\n",
    "Skip gram model allows us to construct a supervised learning task (mapping context word to target word) and how that allows us to learn useful word embeddings. The downside of that was that the softmax objective was slow to compute. In this video, you'll see a modified learning problem called negative sampling ([reference paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)) that allows you to do something similar to the Skip-Gram model you saw just now, but with a much more efficient learning algorithm. \n",
    "\n",
    "We define new learning problem by selecting context target pairs such that for example `context=orange` and `target=juice` is a good pair and `context=king` and `target=king` a bad pair. For a single good pair we choose $k$ bad pairs (it is recommended by the authors that $k$ is in between 5 and 20 for the smaller datasets and between 2 and 5 for larger datasets)\n",
    "<img src=\"imgs/negative_sampling_learning_problem.png\">\n",
    "\n",
    "By defining problem like this we can get rid of the softmax and train simple logistic regression classifier for each word in the vocabulary (for a given context word output can be any word in the vocabulary). And having sampling $k$ negative samples ... this means that on each step we will update parameters for $k+1$ logistic regression parameters ($k$ negative + 1 postive pair)\n",
    "<img src=\"imgs/negative_sampling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe word vectors\n",
    "\n",
    "Glove is yet another algorithm for efficently learning word embeddings.\n",
    "<img src=\"imgs/glove_learning_task.png\">\n",
    "<img src=\"imgs/glove.png\">\n",
    "\n",
    "Note: Glove algorithm can't garantee that embedding features are interpretable\n",
    "\n",
    "References:\n",
    "[GloVe: Global Vectors for Word Representation (2014)](https://nlp.stanford.edu/pubs/glove.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment classification\n",
    "\n",
    "Sentiment classification is the problem of given the piece of text that represents for example a review of a restourant we need to figure out the rating of the review.\n",
    "\n",
    "The naive approach would just take word representations, average them and then passed them to a softmax function which would give probabilities for each review. This approach has a downside which is that it doesnt captures word ordering.\n",
    "\n",
    "<img src=\"imgs/sentiment_classification_avg.png\">\n",
    "\n",
    "Better approach would be to use many to one rnn where word representations would be feed into network.\n",
    "\n",
    "<img src=\"imgs/sentiment_classification_rnn.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiasing word embeddings \n",
    "\n",
    "<img src=\"imgs/bias_problem_in_word_embeddings.png\">\n",
    "\n",
    "In order to debias certain embeddings we need to perform 3 steps:\n",
    "1. Find direction of the bias\n",
    "2. Project embeddings that you want to debias on to non-bias dimensions (doctor, bebysitter)\n",
    "3. Move other embeddings such that they have the same distance from embeddings that you have just debias (grandmother, grandfather)\n",
    "\n",
    "<img src=\"imgs/debiasing_embeddings.png\">\n",
    "\n",
    "References:\n",
    "1. [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings (2016)](https://arxiv.org/abs/1607.06520)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 - Sequence models & Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic models\n",
    "\n",
    "Let say we are building an application for translating from Franch to English. We are given an input sentance in Franch and output should be English translation of that sentance. Papers [1][2] proposed encoder-decoder architecture which takes Franch sentance as an input and then encoder network generates vector representation of that sentance. That vector is then passed to decoder network which generates sentance in English language.\n",
    "\n",
    "<img src=\"imgs/sequence_to_sequence_models.png\">\n",
    "\n",
    "Architecture very similar to this one also works for image captioning where encoder network is CNN (in this example AlexNet) which generates vector representation of the image.\n",
    "\n",
    "<img src=\"imgs/image_captioning.png\">\n",
    "\n",
    "References:\n",
    "1. [Sequence to Sequence Learning with Neural Networks (2014)](https://arxiv.org/abs/1409.3215)\n",
    "2. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (2014)](https://arxiv.org/abs/1406.1078)\n",
    "3. [Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN) (2014)](https://arxiv.org/abs/1412.6632)\n",
    "4. [Show and Tell: A Neural Image Caption Generator (2014)](https://arxiv.org/abs/1411.4555?context=cs.CV)\n",
    "5. [Deep Visual-Semantic Alignments for Generating Image Descriptions (2014)](https://arxiv.org/abs/1412.2306)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking the most likely sentence\n",
    "\n",
    "As we saw in previous lectures language model is estimating probability of the next sentance. Similar to language model \n",
    "machine translation model is estimating conditional probability of the next sentance with respect to input sentance. Therefore, machine translation model is often called conditional language model.\n",
    "\n",
    "Machine translation architecture is very similar to language model architecture except in machine translation architecture encoder network generates the input into the decoder network vs in language model we can feed all zeros vector into the decoder part.\n",
    "\n",
    "<img src=\"imgs/conditional_language_model.png\">\n",
    "\n",
    "Another difference between these two models is that we don't want to sample sentances at random when doing machine translations because we might end up with different translation every time. Instead we want to find most likely translation:\n",
    "\n",
    "<img src=\"imgs/finding_most_likely_translation.png\">\n",
    "\n",
    "Greedy search is the algorithm which would pick up most likely word at each step. If we use greedy search we might end up with having translation like the 2nd sentance on the slide. \n",
    "\n",
    "<img src=\"imgs/greedy_search.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinements to Beam Search\n",
    "\n",
    "How to choose $B$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis in beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bleu Score\n",
    "\n",
    "One of the challenges of machine translation is that, given a French sentence, there could be multiple English translations that are equally good translations of that French sentence. So how do you evaluate a machine translation system if there are multiple equally good answers? BLEU, stands for bilingual evaluation.\n",
    "\n",
    "In the example we are given a Franch sentance and two equally good English translations and let say machine translation outputed the sentance \"the the the ...\". Precision and modified precisions are defined like this:\n",
    "```\n",
    "precision = sum_of_if_the_words_from_mt_appears_in_ref / num_of_words_in_mt\n",
    "modified_precision = max_num_occurences_of_the_words_from_mt_in_all_refs / num_of_words_in_mt\n",
    "```\n",
    "\n",
    "<img src=\"imgs/precision_in_bleu.png\">\n",
    "\n",
    "Similarly, blau score on bigrams:\n",
    "<img src=\"imgs/bleu_score_on_bigrams.png\">\n",
    "\n",
    "More formaly, blau score is:\n",
    "<img src=\"imgs/bleu_score_on_unigrams.png\">\n",
    "\n",
    "Final blau score is defined as average of blau scores achived on n-grams times `BP`. `BP`, or the brevity penalty, is an adjustment factor that penalizes translation systems that output translations that are too short.\n",
    "\n",
    "<img src=\"imgs/bleu_details.png\">\n",
    "\n",
    "References:\n",
    "1. [BLEU: a method for automatic evaluation of machine translation (2002)](https://dl.acm.org/citation.cfm?id=1073135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention model intuition\n",
    "\n",
    "If we have a very long French sentence like this what we are asking this green encoder network to do is, to read in the whole sentence and then memorize the whole sentence and store it in the activations at the last layer of encoder network. Then for the purple network, the decoder network till then generate the English translation based on the input forwarded from the encoder. This is not how humans are translating the sentences (human translators are memorizing one part of the sentance and translate that and then memorize another part of the sentance and translate that etc.) However, this kind of model will actually work but with the long sentances we will see decrease in bleu score as the lenght of sentance grows. \n",
    "\n",
    "<img src=\"imgs/long_sentences.png\">\n",
    "\n",
    "What the Attention model will be computing is a set of attention weights and we're going to use $\\alpha_{11}$ to denote when you're generating the first words, how much should you be paying attention to the first piece of information in first blue circle.\n",
    "\n",
    "<img src=\"imgs/attention_intuition.png\">\n",
    "\n",
    "References:\n",
    "\n",
    "[Neural Machine Translation by Jointly Learning to Align and Translate 2014](https://arxiv.org/abs/1409.0473)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention model\n",
    "\n",
    "<img src=\"imgs/attention_model.png\">\n",
    "\n",
    "<img src=\"imgs/attention_model_2.png\">\n",
    "\n",
    "References:\n",
    "\n",
    "[Neural Machine Translation by Jointly Learning to Align and Translate 2014](https://arxiv.org/abs/1409.0473)\n",
    "[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention 2016](https://arxiv.org/abs/1502.03044)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
