{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 5. Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech recognition (input audio clip, output sequence of words), music generation (no input, output notes), sentiment classification (input sentance, output rating), DNA sequence analysis (input string of AGCT ... output sequence of matching protein), machine translation, video activity recognition, name entity recognition etc.\n",
    "\n",
    "Its also intersting to note that RNN are first introduced in 1986. (https://en.wikipedia.org/wiki/Recurrent_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name entity recognition example:\n",
    "<img src=\"imgs/name_entity_recognition.png\">\n",
    "\n",
    "Notation:\n",
    "\n",
    "$x^{(i)}$ - $i$-th training example \n",
    "\n",
    "$x^{(i)<t>}$ - $t$-th element in the sequence of $i$-th training example\n",
    "\n",
    "$T_{x}^{(i)}$ - length of the $i$-th training example sequence\n",
    "\n",
    "$y^{(i)<t>}$ - $t$-th element of the output sequence of $i$-th training example\n",
    "\n",
    "$T_{y}^{(i)}$ - length of the output sequence of $i$-th training example\n",
    "\n",
    "<img src=\"imgs/representing_words.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "Why not use standard nn for solving for example name entity recognition based on a sentance?\n",
    "1. Input sentances won't always have same length\n",
    "2. Even if we find a way around that (for example define maximum length for an input layer) this type of model doesnt share learned features across different positions of the text \n",
    "\n",
    "The better way to apporach this problem is with reccurent neural nets.\n",
    "<img src=\"imgs/rnn.png\">\n",
    "\n",
    "We define the notation $w_{ax}$ in such that left subscript letter suggest what is being computed - in this example it is activation $a$ and right subscript letter suggest what is the multiplier of the parameters - int this example it is some input vector $x$ \n",
    "\n",
    "<img src=\"imgs/forwardproprnn.png\">\n",
    "\n",
    "In order to simplify notation a bit we would stack $w_{aa}$ and $w_{ax}$ togeather into a $[w_{aa}; w_{ax}] = w_a$. This also implies that we have to stack $a_{t-1}$ and $x_t$ into a single column vector $[a_{t-1}, x_t]$ as well.\n",
    "<img src=\"imgs/simplified_rnn_notation.png\">\n",
    "\n",
    "What we got at the end is following notation:\n",
    "\\begin{align}\n",
    "a_t = g(w_a [a_{t-1}, x] + b_a) \\\\\n",
    "y_t = g(w_y a_t + b_y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation through time\n",
    "\n",
    "After forward propagation is finished (from left to right) we then have to go back and do backprop steps for inputs that happen before and thats why rnn back prop algorithm is called back propagation through time.\n",
    "<img src=\"imgs/back_prop_through_time.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of RNN\n",
    "\n",
    "Reference: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "There are several types of RNN architectures:\n",
    "* one to one - standard nn\n",
    "* one to many - music generation (for a given small set of notes or genre for example nn generates music piece)\n",
    "* many to one - sentiment classification (for example if the movie review is positive or negative)\n",
    "* many to many where $T_x = T_y$ - for example name entity recognition\n",
    "* many to many where $T_x \\neq T_y$ - machine translation\n",
    "\n",
    "<img src=\"imgs/rnn_types3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model and sequence generation\n",
    "\n",
    "If you say \"The apple and pair salad\" vs \"The apple and pear salad\" good speech recognition system should be able to tell the difference and figure out that second sentance has much more sense. The way speech recognition system know what sentance to use is by using a language model ie it assignes each sentance a probability.\n",
    "\n",
    "Language model is a RNN which produces prediction what would be the next word based on a given word. Building language model steps assumes having large corpus and tokenazing text (EOS, UNK additional tokens).\n",
    "\n",
    "<img src=\"imgs/language_model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling novel sequences\n",
    "\n",
    "<img src=\"imgs/sampling_from_lm.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradients with RNNs\n",
    "\n",
    "As with deep neural nets there is a chance that gradients increase (explode) or decrease (vanish) exponentialy. To address exploding gradient problem we can try gradient clipping techinque. To address vanishing gradient problem we are introducing Gated Reccurent Unit ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit GRU (watch again)\n",
    "\n",
    "This is how rnn unit looks like:\n",
    "<img src=\"imgs/rnn_unit.png\">\n",
    "\n",
    "Now lets explain gated recurrent unit (references [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259), [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555)).\n",
    "\n",
    "**The cat, which already ate ... was full**\n",
    "\n",
    "In this example we want for our network to memorize that subject in the sentance (cat) was singular and based on that to generate text ... _was full_ vs _were full_.\n",
    "\n",
    "<img src=\"imgs/gru_unit.png\">\n",
    "\n",
    "Full GRU\n",
    "\n",
    "<img src=\"imgs/full_gru_unit.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory (LSTM)\n",
    "\n",
    "<img src=\"imgs/lstm.png\">\n",
    "\n",
    "Both, GRU and LSTM has their advandages and disadvantages:\n",
    "* GRU is much simpler model so its easier to build much bigger network\n",
    "* LSTM is more powerful and flexible\n",
    "\n",
    "Reference: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN\n",
    "\n",
    "<img src=\"imgs/bidirectional_rnn_1.png\">\n",
    "<img src=\"imgs/bidirectional_rnn_2.png\">\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep RNNs\n",
    "\n",
    "<img src=\"imgs/deep_rnn.png\">\n",
    "\n",
    "Comparing with deep neural nets that we saw in previous lectures having 3 hidden layers for RNN is already huge because of the amount of computations that is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
