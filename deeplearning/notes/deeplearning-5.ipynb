{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 5. Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech recognition (input audio clip, output sequence of words), music generation (no input, output notes), sentiment classification (input sentance, output rating), DNA sequence analysis (input string of AGCT ... output sequence of matching protein), machine translation, video activity recognition, name entity recognition etc.\n",
    "\n",
    "Its also intersting to note that RNN are first introduced in 1986. (https://en.wikipedia.org/wiki/Recurrent_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name entity recognition example:\n",
    "<img src=\"imgs/name_entity_recognition.png\">\n",
    "\n",
    "Notation:\n",
    "\n",
    "$x^{(i)}$ - $i$-th training example \n",
    "\n",
    "$x^{(i)<t>}$ - $t$-th element in the sequence of $i$-th training example\n",
    "\n",
    "$T_{x}^{(i)}$ - length of the $i$-th training example sequence\n",
    "\n",
    "$y^{(i)<t>}$ - $t$-th element of the output sequence of $i$-th training example\n",
    "\n",
    "$T_{y}^{(i)}$ - length of the output sequence of $i$-th training example\n",
    "\n",
    "<img src=\"imgs/representing_words.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "Why not use standard nn for solving for example name entity recognition based on a sentance?\n",
    "1. Input sentances won't always have same length\n",
    "2. Even if we find a way around that (for example define maximum length for an input layer) this type of model doesnt share learned features across different positions of the text \n",
    "\n",
    "The better way to apporach this problem is with reccurent neural nets.\n",
    "<img src=\"imgs/rnn.png\">\n",
    "\n",
    "We define the notation $w_{ax}$ in such that left subscript letter suggest what is being computed - in this example it is activation $a$ and right subscript letter suggest what is the multiplier of the parameters - int this example it is some input vector $x$ \n",
    "\n",
    "<img src=\"imgs/forwardproprnn.png\">\n",
    "\n",
    "In order to simplify notation a bit we would stack $w_{aa}$ and $w_{ax}$ togeather into a $[w_{aa}; w_{ax}] = w_a$. This also implies that we have to stack $a_{t-1}$ and $x_t$ into a single column vector $[a_{t-1}, x_t]$ as well.\n",
    "<img src=\"imgs/simplified_rnn_notation.png\">\n",
    "\n",
    "What we got at the end is following notation:\n",
    "\\begin{align}\n",
    "a_t = g(w_a [a_{t-1}, x] + b_a) \\\\\n",
    "y_t = g(w_y a_t + b_y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation through time\n",
    "\n",
    "After forward propagation is finished (from left to right) we then have to go back and do backprop steps for inputs that happen before and thats why rnn back prop algorithm is called back propagation through time.\n",
    "<img src=\"imgs/back_prop_through_time.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of RNN\n",
    "\n",
    "Reference: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "There are several types of RNN architectures:\n",
    "* one to one - standard nn\n",
    "* one to many - music generation (for a given small set of notes or genre for example nn generates music piece)\n",
    "* many to one - sentiment classification (for example if the movie review is positive or negative)\n",
    "* many to many where $T_x = T_y$ - for example name entity recognition\n",
    "* many to many where $T_x \\neq T_y$ - machine translation\n",
    "\n",
    "<img src=\"imgs/rnn_types3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model and sequence generation\n",
    "\n",
    "If you say \"The apple and pair salad\" vs \"The apple and pear salad\" good speech recognition system should be able to tell the difference and figure out that second sentance has much more sense. The way speech recognition system know what sentance to use is by using a language model ie it assignes each sentance a probability.\n",
    "\n",
    "Language model is a RNN which produces prediction what would be the next word based on a given word. Building language model steps assumes having large corpus and tokenazing text (EOS, UNK additional tokens).\n",
    "\n",
    "<img src=\"imgs/language_model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling novel sequences\n",
    "\n",
    "<img src=\"imgs/sampling_from_lm.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradients with RNNs\n",
    "\n",
    "As with deep neural nets there is a chance that gradients increase (explode) or decrease (vanish) exponentialy. To address exploding gradient problem we can try gradient clipping techinque. To address vanishing gradient problem we are introducing Gated Reccurent Unit ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit GRU (watch again)\n",
    "\n",
    "This is how rnn unit looks like:\n",
    "<img src=\"imgs/rnn_unit.png\">\n",
    "\n",
    "Now lets explain gated recurrent unit (references [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259), [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555)).\n",
    "\n",
    "**The cat, which already ate ... was full**\n",
    "\n",
    "In this example we want for our network to memorize that subject in the sentance (cat) was singular and based on that to generate text ... _was full_ vs _were full_.\n",
    "\n",
    "<img src=\"imgs/gru_unit.png\">\n",
    "\n",
    "Full GRU\n",
    "\n",
    "<img src=\"imgs/full_gru_unit.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory (LSTM)\n",
    "\n",
    "<img src=\"imgs/lstm.png\">\n",
    "\n",
    "Both, GRU and LSTM has their advandages and disadvantages:\n",
    "* GRU is much simpler model so its easier to build much bigger network\n",
    "* LSTM is more powerful and flexible\n",
    "\n",
    "Reference: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN\n",
    "\n",
    "<img src=\"imgs/bidirectional_rnn_1.png\">\n",
    "<img src=\"imgs/bidirectional_rnn_2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep RNNs\n",
    "\n",
    "<img src=\"imgs/deep_rnn.png\">\n",
    "\n",
    "Comparing with deep neural nets that we saw in previous lectures having 3 hidden layers for RNN is already huge because of the amount of computations that is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 - Natural Language Processing & Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word representation\n",
    "\n",
    "Explanation of what is word embedding:\n",
    "<img src=\"imgs/word_embeddings.png\">\n",
    "\n",
    "There are several commonly used algorithms for visualising word embeddings (projecting the word vectors on 2D plane) and one of them is [t-sne](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word embeddings\n",
    "\n",
    "<img src=\"imgs/transfer_learning_word_embeddings.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of word embeddings\n",
    "\n",
    "Reference https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf\n",
    "This paper presented idea of using embeddings differences to figure out analogies between the words. Important thing to note is that after applying t-sne to embeddings (after projecting them on 2D plane) embeddings lose the analogies info\n",
    "<img src=\"imgs/analogies.png\">\n",
    "<img src=\"imgs/cosine_similarity.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix\n",
    "\n",
    "Embedding matrix is matrix in which columns are word vectors. Size of embedding matrix is `embedding_dim` x `vocabulary_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning word embeddings\n",
    "\n",
    "We are solving the problem of language modeling and for a given a sequence of words we want to predict next word. This can be done like this:\n",
    "1. randomly initialize matrix of embeddings $E$\n",
    "2. stack $k$ word embeddings into a matrix and feed them into the neural network (in this example $k = 6$)\n",
    "3. neural network will have softmax at the end which would give probabilities of what is the next word in the sequence for each word in the vocabulary ([reference paper](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf))\n",
    "\n",
    "<img src=\"imgs/natural_language_model.png\">\n",
    "\n",
    "So, what we saw on the last slide was that the job of the algorithm was to predict some word `juice`, which we are going to call the **target** words, and it was given some **context** which was the last four words. And so, if your goal is to learn a embedding of researchers have experimented with many different types of context. \n",
    "\n",
    "<img src=\"imgs/context_target_pairs.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec skip gram model\n",
    "\n",
    "Word2vec is almost the same algorithm as the previous one, only the context is choosen differently. In word2vec model contex is a single word left of the target word. Word2vec model is described in [this](https://arxiv.org/abs/1301.3781) paper. Very important thing to note is that the goal of skip gram word2vec model is not to perform well on the task (given a single context word predict the next word) but to efficiently learn word representations.\n",
    "\n",
    "Note: Usual loss for softmax unit is negative log-likelyhood $L(y, \\hat{y}) = -\\sum_{j=1}^{10000}y_ilog\\hat{y_i}$\n",
    "\n",
    "<img src=\"imgs/skip_grams.png\">\n",
    "<img src=\"imgs/word2vec.png\">\n",
    "<img src=\"imgs/hiararchical_softmax.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling\n",
    "\n",
    "Skip gram model allows us to construct a supervised learning task (mapping context word to target word) and how that allows us to learn useful word embeddings. The downside of that was that the softmax objective was slow to compute. In this video, you'll see a modified learning problem called negative sampling ([reference paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)) that allows you to do something similar to the Skip-Gram model you saw just now, but with a much more efficient learning algorithm. \n",
    "\n",
    "We define new learning problem by selecting context target pairs such that for example `context=orange` and `target=juice` is a good pair and `context=king` and `target=king` a bad pair. For a single good pair we choose $k$ bad pairs (it is recommended by the authors that $k$ is in between 5 and 20 for the smaller datasets and between 2 and 5 for larger datasets)\n",
    "<img src=\"imgs/negative_sampling_learning_problem.png\">\n",
    "\n",
    "By defining problem like this we can get rid of the softmax and train simple logistic regression classifier for each word in the vocabulary (for a given context word output can be any word in the vocabulary). And having sampling $k$ negative samples ... this means that on each step we will update parameters for $k+1$ logistic regression parameters ($k$ negative + 1 postive pair)\n",
    "<img src=\"imgs/negative_sampling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe word vectors\n",
    "\n",
    "[Glove](https://nlp.stanford.edu/pubs/glove.pdf) is yet another algorithm for efficently learning word embeddings.\n",
    "<img src=\"imgs/glove_learning_task.png\">\n",
    "<img src=\"imgs/glove.png\">\n",
    "\n",
    "Note: Glove algorithm can't garantee that embedding features are interpretable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment classification\n",
    "\n",
    "Sentiment classification is the problem of given the piece of text that represents for example a review of a restourant we need to figure out the rating of the review.\n",
    "\n",
    "The naive approach would just take word representations, average them and then passed them to a softmax function which would give probabilities for each review. This approach has a downside which is that it doesnt captures word ordering.\n",
    "\n",
    "<img src=\"imgs/sentiment_classification_avg.png\">\n",
    "\n",
    "Better approach would be to use many to one rnn where word representations would be feed into network.\n",
    "\n",
    "<img src=\"imgs/sentiment_classification_rnn.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiasing word embeddings \n",
    "\n",
    "<img src=\"imgs/bias_problem_in_word_embeddings.png\">\n",
    "\n",
    "In order to debias certain embeddings we need to perform 3 steps:\n",
    "1. Find direction of the bias\n",
    "2. Project embeddings that you want to debias on to non-bias dimensions (doctor, bebysitter)\n",
    "3. Move other embeddings such that they have the same distance from embeddings that you have just debias (grandmother, grandfather)\n",
    "\n",
    "<img src=\"imgs/debiasing_embeddings.png\">\n",
    "\n",
    "[Reference](https://arxiv.org/abs/1607.06520)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
