{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1. Build Basic Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "* week 1 & 2 - Basics\n",
    "* week 3 - W-loss\n",
    "* week 4 - Controling the output\n",
    "\n",
    "## Weeks 1 and 2. Basics\n",
    "\n",
    "* Two type of models Discriminative and Generative\n",
    "    1. Discrimitative models (classifiers), image X -> image class Y, modeling distribution P(Y|X)\n",
    "    2. Generative models: noise, image class Y -> image X, modeling distribution P(X|Y)\n",
    "\n",
    "\n",
    "* Two types of generative models VAE and GANs\n",
    "\n",
    "<img src=\"imgs/vae.png\">\n",
    "<img src=\"imgs/gans.png\">\n",
    "\n",
    "* Training GANs\n",
    "<img src=\"imgs/training_discriminator.png\">\n",
    "<img src=\"imgs/training_generator.png\">\n",
    "\n",
    "\n",
    "GANs take a lot of time to train, especially if you want to build them for\n",
    "really cool applications like the one discussing this course.\n",
    "GANs are often quite fragile when they learn to because they aren't as\n",
    "straightforward as a classifier.\n",
    "And sometimes the skills of the generator and\n",
    "discriminator aren't as aligned as they could be.\n",
    "For these reasons, every trick that speeds up in stabilizes training is crucial for\n",
    "these models, and batch normalization has proven very effective to that end. \n",
    "\n",
    "* Batch norm (Covariant shift ???)\n",
    "<img src=\"imgs/batch_norm1.png\">\n",
    "<img src=\"imgs/batch_norm_training.png\">\n",
    "<img src=\"imgs/batch_norm_testing.png\">\n",
    "\n",
    "* Two upsampling techniques\n",
    "    1. upsampling without learnable parametars\n",
    "<img src=\"imgs/upsampling.png\">\n",
    "    2. upsampling with learnable parametars - transposed convolution\n",
    "<img src=\"imgs/transposed_conv.png\">\n",
    "<img src=\"imgs/transpose_conv_problem.png\">\n",
    "\n",
    "\n",
    "## Week 3. W-loss\n",
    "# WEEK 3 VERY IMPORTANT - REPEAT\n",
    "\n",
    "Another major issue with GAN training is that happens that GAN is generating the same thing each time.\n",
    "So a GAN trained on\n",
    "all different dog breeds\n",
    "will only generate a golden retriever.\n",
    "That's obviously an issue.\n",
    "This issue happens because\n",
    "the discriminator improves but it gets\n",
    "stuck between saying an image of a dog looks\n",
    "extremely fake or extremely real.\n",
    "It's a classifier after all,\n",
    "so it's encouraged to say it's one\n",
    "real or zero fake as it gets better.\n",
    "But in a single round of training,\n",
    "if the discriminator only thinks\n",
    "the generator's golden retriever looks real,\n",
    "even if it doesn't even look that real,\n",
    "then the generator will cling on to\n",
    "that golden retriever and only produce golden retrievers.\n",
    "Now when the discriminator learns that\n",
    "this golden retriever is\n",
    "fake in the next round of training,\n",
    "the generator won't know where to\n",
    "go because there's really nothing else it\n",
    "has in its arsenal of\n",
    "different images and that's the end of learning.\n",
    "The end of learning is very,\n",
    "very bad for these networks.\n",
    "Digging one level deeper,\n",
    "this happens because of binary cross-entropy loss,\n",
    "where the discriminator is forced to\n",
    "produce a value between zero or one,\n",
    "and even though there's\n",
    "an infinite number of\n",
    "decimal values between zero and one,\n",
    "it'll approach zero and one as it gets better.\n",
    "This week you'll be\n",
    "introduced to a new loss function that\n",
    "allows a discriminator to say negative four or 100,\n",
    "any number between negative infinity and infinity,\n",
    "which mitigates this problem and allows\n",
    "both networks to keep on learning just like you.\n",
    "Your assignment will be implementing this\n",
    "amazing GAN upgrade.\n",
    "\n",
    "* **Two problems with GAN training 1. Mode collapse 2. vanishing gradients**\n",
    "\n",
    "* Mode - any peak on probabilty density function is call mode\n",
    "<img src=\"imgs/mode_def.png\">\n",
    "\n",
    "* Mode collapse happens when the generator learns to fool the discriminator by producing\n",
    "examples from a single class from\n",
    "the whole training dataset like handwritten number ones.\n",
    "This is unfortunate because,\n",
    "while the generator is\n",
    "optimizing to fool the discriminator,\n",
    "that's not what you ultimately want your generator to do. \n",
    "\n",
    "\n",
    "* Minimax game and vanishing gradient\n",
    "\n",
    "The generator wants to maximize\n",
    "this cost because that means the discriminator is\n",
    "doing poorly and is classifying\n",
    "it's fake values into reals.\n",
    "Whereas the discriminator wants to minimize\n",
    "this cost function because that means\n",
    "it's classifying things correctly\n",
    "<img src=\"imgs/minmax_game.png\">\n",
    "\n",
    "* Earth mover's distance\n",
    "\n",
    "When using BCE loss to train a GAN,\n",
    "you often encounter mode collapse,\n",
    "and vanishing gradient problems due to\n",
    "the underlying cost function of the whole architecture.\n",
    "Even though there is an infinite number\n",
    "of decimal values between zero and one,\n",
    "the discriminator, as it improves,\n",
    "will be pushing towards those ends.\n",
    "In this video, you'll see\n",
    "a different underlying cost function\n",
    "called Earth mover's distance,\n",
    "that measures the distance between\n",
    "two distributions and generally\n",
    "outperforms the one associated\n",
    "with BCE loss for training GANs. \n",
    "\n",
    "* W-loss\n",
    "* W-loss condition (1Lipschitz)\n",
    "* Methods for enforcing the condition during the training\n",
    "\n",
    "## Week 4. Conditional GANs & Controllable Generation\n",
    "\n",
    "Unconditional GAN - input noise, output image of a random dog\n",
    "\n",
    "<img src=\"imgs/uncoditional_generation.png\">\n",
    "\n",
    "Conditional GAN - input noise and class (husky), output image of a husky \n",
    "\n",
    "<img src=\"imgs/conditional_generation.png\">\n",
    "\n",
    "<img src=\"imgs/conditional_vs_unconditional.png\">\n",
    "<img src=\"imgs/conditional_generator_input.png\">\n",
    "<img src=\"imgs/conditional_D_input1.png\">\n",
    "<img src=\"imgs/conditional_D_input2.png\">\n",
    "\n",
    "* Controllable Generation - is done after training by modifying the z-vectors passed to the generator while conditional generation is done during training and requires a labelled dataset.\n",
    "\n",
    "<img src=\"imgs/controllable_generation.png\">\n",
    "<img src=\"imgs/controllable_generation2.png\">\n",
    "<img src=\"imgs/z-space.png\">\n",
    "<img src=\"imgs/controllable_generation_challanges.png\">\n",
    "\n",
    "## IMPORTANT FOR LF PROJECT (disentagled Z space)\n",
    "<img src=\"imgs/classifer_gradients.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 2. Build Better Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* week 1 - Gan evaluation (FID, IS)\n",
    "* week 2 - GAN Disadvantages and Bias\n",
    "* week 3 - StyleGan\n",
    "\n",
    "## Week 3. StyleGan\n",
    "\n",
    "Main improvements of GANs over time related to **stability in training**:\n",
    "\n",
    "1. standard deviation on minibatch ??\n",
    "2. to ensure 1-Lipschitz continuity W-loss with gradient penalty is used\n",
    "3. to ensure 1-Lipschitz continuity Spectral Normalization (similar to batch normalization, available in pytorch) is used\n",
    "4. moving average - taking average weights of the generator ??\n",
    "5. progressive growing - it gradually trains your generator on increasing image resolutions\n",
    "\n",
    "Main improvements of GANs over time related to **capacity** (image resolution) is mostly due to better hardware.\n",
    "Main improvements of GANs over time related to **diversity** is mostly due to more diverse datasets but also 2 technical methods introduced by StyleGan\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
