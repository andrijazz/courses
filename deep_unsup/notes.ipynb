{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS294 - Deep Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5. Implicit models and GANs\n",
    "\n",
    "### Motivation\n",
    "\n",
    "<img src=\"imgs/motivation.png\">\n",
    "\n",
    "Lets say we want to build the sampler. The simplest possible way would be to just randomly take samples from the training data:\n",
    "\n",
    "### Original GAN\n",
    "\n",
    "<img src=\"imgs/sampler1.png\">\n",
    "\n",
    "But we donâ€™t just want to sample the exact data points you have. We want to build a generative model that can:\n",
    "* understand the underlying distribution of data points and smoothly interpolate across the training samples\n",
    "* output samples similar but not the same as training data samples\n",
    "* output samples representative of the underlying factors of variation in the training distribution. \n",
    "\n",
    "Example: digits with unseen strokes, faces with unseen poses, etc. \n",
    "\n",
    "_def_. **Implicit models**\n",
    "\n",
    "<img src=\"imgs/implicit_model_def.png\">\n",
    "\n",
    "_def_. **GANs**\n",
    "\n",
    "<img src=\"imgs/gans_cost_func.png\">\n",
    "<img src=\"imgs/gans_tutorial.png\">\n",
    "\n",
    "### Evaluation metrics\n",
    "\n",
    "1. Parzen-Window density estimator\n",
    "\n",
    "2. Inception score\n",
    "    \n",
    "3. Frechet Inception Distance\n",
    "\n",
    "\n",
    "### Theory behind GANs\n",
    "\n",
    "### Progression of GANs\n",
    "\n",
    "**DCGAN** (Deep Convolutional GANs) propose GANs as a way to learn feature representations and they show that those representations are actually capturing important aspects of the images.\n",
    "\n",
    "This is the DCGAN generator architecture which takes 100 dim random Gaussian noise which is forwarded through dense layer and reshaped to make the tensor spatial. The non-linearity at the end is $tanh$ and we are forcing $[-1, 1]$ values because we want to make pixels out of those values. We would then simply create pixels by doing $(out + 1) * 127.5 = rgbout$. \n",
    "<img src=\"imgs/dcgan1.png\">\n",
    "<img src=\"imgs/dcgan2.png\">\n",
    "One important point that batch norm should be used on real and generated tensors separately.\n",
    "<img src=\"imgs/dcgan3.png\">\n",
    "<img src=\"imgs/dcgan4.png\">\n",
    "\n",
    "**Improved training of GANs**\n",
    "\n",
    "**WGAN** introduced moving away from binary classifier in $D$. In WGAN, $D = f_w$ outputs a single scalar value and $f_w$ has to be aproximatelly Lipschitz ??????? Weight clipping step assures that Lipschitzness.\n",
    "<img src=\"imgs/wgan1.png\">\n",
    "<img src=\"imgs/wgan2.png\">\n",
    "\n",
    "\n",
    "**WGAN-GP** improved way to ensure weight clipping by introducing regularization term.\n",
    "<img src=\"imgs/wgan-gp1.png\">\n",
    "<img src=\"imgs/wgan-gp2.png\">\n",
    "\n",
    "Positive and negative (underlined with red pen) sides of WGAN-GP:\n",
    "<img src=\"imgs/wgan-gp3.png\">\n",
    "\n",
    "\n",
    "**Progressive GAN**\n",
    "...\n",
    "\n",
    "**Style GAN** proposed novel idea to move away from passing random noise $z$ directly into the Generator. The idea is to always start from the constant and pass various transformations of $z$ (called style vectors $w$) on the different layers of the generator. During the training they were also adding gaussian random noise at various levels (this is a common trick when training GANs)\n",
    "\n",
    "<img src=\"imgs/stylegan1.png\">\n",
    "\n",
    "The crucial trick of style gan is how they apply style vectors to various layers and thats achived with **Adaptive Instance Norm (AdaIN)**.\n",
    "<img src=\"imgs/stylegan2.png\">\n",
    "\n",
    "\n",
    "**Style GAN v2** is fixing the problem of waterdrops like artifacts that style gan v1 has.\n",
    "\n",
    "<img src=\"imgs/styleganv21.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_inception_score(p_yx, eps=1E-16):\n",
    "    # calculate p(y)\n",
    "    p_y = expand_dims(p_yx.mean(axis=0), 0)\n",
    "    # kl divergence for each image\n",
    "    kl_d = p_yx * (log(p_yx + eps) - log(p_y + eps))\n",
    "    # sum over classes\n",
    "    sum_kl_d = kl_d.sum(axis=1)\n",
    "    # average over images\n",
    "    avg_kl_d = mean(sum_kl_d)\n",
    "    # undo the logs\n",
    "    is_score = exp(avg_kl_d)\n",
    "    return is_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
