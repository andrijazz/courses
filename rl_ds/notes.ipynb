{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning course by David Silver notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2. Markov Decision Processes\n",
    "\n",
    "* What is Random process?\n",
    "* What is Markov property?\n",
    "* What is state transition matrix?\n",
    "* What is Markov process or Markov chain?\n",
    "* Draw an example diagram of Markov process?\n",
    "* What is Markov reward process? (Reward function takes state s and returns expected reward, Value-function takes state s and returns expected return - long term value of being in a state)\n",
    "* Draw an example of Markov reward process?\n",
    "* What is return?\n",
    "* What is value function in Markov reward process?\n",
    "* Bellman equation for MRP?\n",
    "* What is backup diagram in MRP + example?\n",
    "* What is MDP?\n",
    "* Draw an example of MDP?\n",
    "* What is the policy?\n",
    "* What is state-value function?\n",
    "* What is action-value function?\n",
    "* Bellman expectation equations?\n",
    "* What is optimal policy?\n",
    "* Bellman optimality equations?\n",
    "\n",
    "TODO: episode, dynamics of MDP, example diagram of MDP\n",
    "\n",
    "### Definitions\n",
    "\n",
    "MDP is the framework that enables us to model real world problems and formalize that problem mathematicaly. In order to understand MDPs it is crucial to fully understand every term that we are about to define and because of that this note will be in the form of cheetsheet with lots of definitions.\n",
    "\n",
    "<img src=\"imgs/agent-env.png\">\n",
    "\n",
    "_def_. Real world problem that we want to model we call **Stochastic or Random process** in the language of statistics. We can think of a random process as a set of random states. More formaly, stochastic process is a family of random variables describing certain events.\n",
    "\n",
    "_def_. If the next state of a process depends only on present state and not on previous states we say that process has **Markov property**. This should be thought as a restictions of the states meaning that each state should capture all relevant information from history. More formaly, random process with states $S_1 ... S_n$ satisfies Markov property if:\n",
    "\n",
    "$$\n",
    "P(S_{t+1}|S_t) = P(S_{t+1}|S_1, ... ,S_{t})\n",
    "$$\n",
    "\n",
    "for every state.\n",
    "\n",
    "_def_. **State transition matrix** is a square matrix which tell us what is the probabilty of transitioning from one state to another.\n",
    "\n",
    "$$\n",
    "\\mathcal{P} = \\begin{bmatrix}\n",
    "p_{11} & .. & p_{1n}\\\\\n",
    ". &  & \\\\\n",
    ". &  & \\\\\n",
    "p_{n1} & .. & p_{nn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "_def_. **Markov process** or **Markov chain** is a process with Markov property ie. sequence of random states $S_1, S_2 ...$ with Markov property. More formaly, Markov process is a tuple $\\langle \\mathcal{S}, \\mathcal{P} \\rangle$ where $\\mathcal{S}$ is set of states and $\\mathcal{P}$ is state transition matrix.\n",
    "\n",
    "<img src=\"imgs/mp.png\">\n",
    "\n",
    "<img src=\"imgs/stochastic-process-not-markov.gif\">\n",
    "<img src=\"imgs/stochastic-process-is-markov.gif\">\n",
    "\n",
    "_def_. **Markov reward process** is a Markov process with value judgements - how good it is to be in a certain stated. Markov reward process is a tuple $\\langle \\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\gamma \\rangle$ where $\\mathcal{S}$ is set of states, $\\mathcal{P}$ is state transition matrix, $\\mathcal{R}$ is reward function and $\\gamma$ is a discount factor.\n",
    "\n",
    "<img src=\"imgs/mrp.png\">\n",
    "\n",
    "\n",
    "_def_. **Return** $G_t$ is a total dicounted sum of rewards you get after time $t$.\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} ... = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "_def_. The **goal** is a optimal return. The goal of reinforcement learning is to optimize return.\n",
    "\n",
    "_def_. **Value function** $v(s)$ gives the long-term value of being in a state (expectation)\n",
    "\n",
    "<img src=\"imgs/value_functions.png\">\n",
    "\n",
    "_def_. **Bellman equation for MRPs** is following equation and it basically states that value function can be decomposed into 2 parts: 1. imidiate reward $R_{t+1}$ 2. discounted value of successor state $\\gamma v(S_{t+1})$\n",
    "\n",
    "<img src=\"imgs/bellman_mrps.png\">\n",
    "<img src=\"imgs/IMG_5525.jpg\">\n",
    "\n",
    "_def_. **Backup diagram** is one-step look ahead tree which helps us visualize one step of a process.\n",
    "\n",
    "<img src=\"imgs/calc_value_function.png\">\n",
    "\n",
    "<img src=\"imgs/bellman_eq_mat.png\">\n",
    "\n",
    "Because Bellman equation is linear it is possible to be solved (calculate $v$) directly:\n",
    "\n",
    "<img src=\"imgs/solve_bellman_eq.png\">\n",
    "\n",
    "Computational complexity of this calculation is $O(n^3)$ and therefore direct solution is applicable only to small MRPs. For larger MRPs other methods are avalible: Dynamic programming, Monte-carlo evaluation and Temporal-Difference learning.\n",
    "\n",
    "_def_. **Markov decision process**\n",
    "\n",
    "<img src=\"imgs/mdp.png\">\n",
    "\n",
    "_def_. **Policy** is a mapping from states to probabilities of selecting each possible action.\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = P(A_t=a|S_t=s)\n",
    "$$\n",
    "\n",
    "<img src=\"imgs/mdp_note.png\">\n",
    "\n",
    "_def_. Previously, in MRPs, we defined a value function $v(s)$ as a long-term value of being in a state. In MDPs we are defining **state-value function**  which tells us how good is to be in the state following the policy $\\pi$.State-value function is defined as expected return starting from $s$ following policy $\\pi$.\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = E_{\\pi}(G_t|S_t=s)\n",
    "$$\n",
    "\n",
    "_def_. **Action-value function** tells us how good is to take certain action from a peticular state while following the policy $\\pi$. We defined action-value function $q_{\\pi}(s, a)$ as a expected return when starting from state $s$, taking the action $a$ while following policy $\\pi$:\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = E_{\\pi}(G_t|S_t=s, A_t=a)\n",
    "$$\n",
    "\n",
    "\n",
    "We can now also define how Bellman equation looks like in MDPs.\n",
    "\n",
    "_def_. **Bellman expectation equation for state-value function**:\n",
    "\n",
    "<img src=\"imgs/bellman_eq_sv.png\">\n",
    "\n",
    "<!--- \n",
    "$$\n",
    "\\mathsf{v}_{\\pi}(s) = \\mathbb{E}_{\\pi}(R_{t+1} + \\gamma\\mathsf{v}_{\\pi}(S_{t+1}) | S_t = s) = \\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s, a)\n",
    "$$\n",
    " -->\n",
    "_def_. **Bellman expectation equation for action-value function**:\n",
    "<img src=\"imgs/bellman_eq_av.png\">\n",
    "<!-- \n",
    "$$\n",
    "q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}(R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a) = R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{ss'}^{a}\\mathsf{v}_{\\pi}(s')\n",
    "$$\n",
    " -->\n",
    " \n",
    "Therefore,\n",
    "<img src=\"imgs/e_bellman_eq1.png\">\n",
    "<img src=\"imgs/e_bellman_eq2.png\">\n",
    "<img src=\"imgs/e_bellman_eq3.png\">\n",
    "<img src=\"imgs/e_bellman_eq4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimality\n",
    "\n",
    "_def_. **Optimal value functions**\n",
    "<img src=\"imgs/optimal_value_functions.png\">\n",
    "\n",
    "_def_. **Optimal policy**\n",
    "<img src=\"imgs/optimal_policy.png\">\n",
    "\n",
    "_def_. **Bellman optimality equation**\n",
    "<img src=\"imgs/o_bellman_eq1.png\">\n",
    "<img src=\"imgs/o_bellman_eq2.png\">\n",
    "<img src=\"imgs/o_bellman_eq3.png\">\n",
    "<img src=\"imgs/o_bellman_eq4.png\">\n",
    "\n",
    "### Example\n",
    "<img src=\"imgs/recycling_robot_1.jpg\">\n",
    "<img src=\"imgs/recycling_robot_2.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3. Planning by Dynamic Programming\n",
    "\n",
    "* What is DP?\n",
    "* What is planning problem?\n",
    "* What is prediction and what is control?\n",
    "* What is policy evaluation?\n",
    "* Write down iterative policy evaluation algorithm?\n",
    "* Formulate and prove policy improvement theorem?\n",
    "* Write down policy iteration algorithm?\n",
    "* What is modified policy iteration?\n",
    "* What is generalized policy iteration?\n",
    "* Formulate priciple of optimality?\n",
    "* Describe value iteration algorithm?\n",
    "* Sync vs async dp?\n",
    "\n",
    "**Dynamic programming** is the method for solving complex problems by 1. breaking them down into subproblems -> 2. solve the subproblems -> 3. combine solutions to subproblems\n",
    "\n",
    "Dynamic Programming is a very general solution method for problems which have two properties:\n",
    "1. Optimal substructure\n",
    "    * Principle of optimality applies\n",
    "    * Optimal solution can be decomposed into subproblems\n",
    "2. Overlapping subproblems\n",
    "    * Subproblems recur many times\n",
    "    * Solutions can be cached and reused\n",
    "    \n",
    "MDPs satisfy both properties - Bellman equation gives recursive decomposition and value function stores and reuses solutions (value functions are *cache*).\n",
    "\n",
    "_def_. **Planning** is the problem of solving the MDP (finding the optimal policy) given the full knowledge about  MDP (we know rewards and dynamics). DP is used for solving a planning problem. Solving the planning can be split into 2 steps:\n",
    "1. **Prediction** is the problem of evaluating the policy $\\pi$\n",
    "2. **Control** is the problem of finding the optimal policy.\n",
    "\n",
    "<img src=\"imgs/prediction_and_control.png\">\n",
    "\n",
    "_def_. **Policy evaluation** is the process of figuring out how to evaluate the policy ie. if someone gives us the policy we need to figure out how much reward we are going to collect when following that policy. Policy evaluation is iterative algorithm based on Bellman expectation equation.\n",
    "\n",
    "<img src=\"imgs/policy_eval.png\">\n",
    "\n",
    "Note that value function $V$ is the 1-dim array (cache) of values of each state.\n",
    "\n",
    "<img src=\"imgs/iterative_policy_eval_bkp_diag.png\">\n",
    "\n",
    "_def_. **Policy iteration** is the process of finding the optimal policy. Policy iteration is iterative algorithm based on Bellman optimality equation. Policy iteration consist of two steps that are repeating in a cycle, policy evaluation and policy improvement.\n",
    "\n",
    "\n",
    "<img src=\"imgs/policy_iter.png\">\n",
    "\n",
    "_def_. **Policy improvement** is the process of improving the policy. We are starting from policy $\\pi$ and the way we come up with the new policy is to act greedly with respect to current action-value function. More formally,\n",
    "\n",
    "$$\n",
    "\\pi'(a|s) = \\underset{a \\in A}{\\operatorname{argmax}}q_{\\pi}(s, a)\n",
    "\\tag{1}    \n",
    "$$\n",
    "\n",
    "\n",
    "Lets explain the following theorem by focusing on a special case of *determinstic policy* $a = \\pi(s)$. The theorem is easily expandable to the case of stochastic policies $\\pi(a|s)$.\n",
    "\n",
    "\n",
    "_theorem_. (**Policy improvement theorem**) Let $\\pi$ and $\\pi'$ be any pair of deterministic policies such that, for all $s \\in S$ \n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, \\pi'(s)) \\ge v_{\\pi}(s)\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "Then the policy $\\pi'$ must be as good as, or better then, $\\pi$. That is, it must obtain greater or equal expected return from all states $s \\in S$\n",
    "\n",
    "$$\n",
    "v_{\\pi'}(s) \\ge v_{\\pi}(s).\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "$\\Delta.$ TODO\n",
    "\n",
    "This means that if we are acting greedly (like described by equation (1)) then the condition for policy improvement theorem (equation (2)) is fullfiled so we can apply the theorem and conclude that updating policy in the greedy manner will indeed improve policy. Another conclusion is that if improvement stops that means that we reached optimal policy.\n",
    "\n",
    "\n",
    "<img src=\"imgs/policy_iteration_2.png\">\n",
    "\n",
    "\n",
    "_def_. **Modified policy iteration** is essential the same algorithm as policy iteration except it can have several variations:\n",
    "1. We can introduce stopping trashold $\\epsilon$ and then not wait to find exact optimal policy until the end of the loop, but instead stop a little earilier (while ($\\pi' - \\pi) > \\epsilon$)\n",
    "2. Or simply stop after $k$ iterations of iterative policy evaluation\n",
    "\n",
    "So far, we were discussing *policy iteration* algorithm by combining *iterative policy evaluation* and *greedy policy improvement*. **Generalized policy iteration** is the same approach except we can combine **any** policy evaluation algorithm and **any** policy improvement algorithm.\n",
    "\n",
    "<img src=\"imgs/generalized_policy_iteration.png\">\n",
    "\n",
    "_theorem_. **Principle of optimality**\n",
    "<img src=imgs/principle_of_optimality.png>\n",
    "\n",
    "_def_. **Value iteration**\n",
    "\n",
    "<img src=\"imgs/deterministic_value_iter.png\">\n",
    "<img src=\"imgs/value_iter1.png\">\n",
    "<img src=\"imgs/value_iter2.png\">\n",
    "<img src=\"imgs/value_iteration.png\">\n",
    "\n",
    "<img src=\"imgs/sync_dp.png\">\n",
    "\n",
    "_def_. **Asynchronous dynamic programming** TODO\n",
    "\n",
    "TODO other ideas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
