{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning course by David Silver notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2. Markov Decision Processes\n",
    "\n",
    "* What is Random process?\n",
    "* What is Markov property?\n",
    "* What is state transition matrix?\n",
    "* What is Markov process or Markov chain?\n",
    "* Draw an example diagram of Markov process?\n",
    "* What is Markov reward process? (Reward function takes state s and returns expected reward, Value-function takes state s and returns expected return - long term value of being in a state)\n",
    "* Draw an example of Markov reward process?\n",
    "* What is return?\n",
    "* What is value function in Markov reward process?\n",
    "* Bellman equation for MRP?\n",
    "* What is backup diagram in MRP + example?\n",
    "* What is MDP?\n",
    "* Draw an example of MDP?\n",
    "* What is the policy?\n",
    "* What is state-value function?\n",
    "* What is action-value function?\n",
    "* Bellman expectation equations?\n",
    "* What is optimal policy?\n",
    "* Bellman optimality equations?\n",
    "\n",
    "TODO: episode, dynamics of MDP, example diagram of MDP\n",
    "\n",
    "### Definitions\n",
    "\n",
    "MDP is the framework that enables us to model real world problems and formalize that problem mathematicaly. In order to understand MDPs it is crucial to fully understand every term that we are about to define and because of that this note will be in the form of cheetsheet with lots of definitions.\n",
    "\n",
    "<img src=\"imgs/agent-env.png\">\n",
    "\n",
    "_def_. Real world problem that we want to model we call **Stochastic or Random process** in the language of statistics. We can think of a random process as a set of random states. More formaly, stochastic process is a family of random variables describing certain events.\n",
    "\n",
    "_def_. If the next state of a process depends only on present state and not on previous states we say that process has **Markov property**. This should be thought as a restictions of the states meaning that each state should capture all relevant information from history. More formaly, random process with states $S_1 ... S_n$ satisfies Markov property if:\n",
    "\n",
    "$$\n",
    "P(S_{t+1}|S_t) = P(S_{t+1}|S_1, ... ,S_{t})\n",
    "$$\n",
    "\n",
    "for every state.\n",
    "\n",
    "_def_. **State transition matrix** is a square matrix which tell us what is the probabilty of transitioning from one state to another.\n",
    "\n",
    "$$\n",
    "\\mathcal{P} = \\begin{bmatrix}\n",
    "p_{11} & .. & p_{1n}\\\\\n",
    ". &  & \\\\\n",
    ". &  & \\\\\n",
    "p_{n1} & .. & p_{nn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "_def_. **Markov process** or **Markov chain** is a process with Markov property ie. sequence of random states $S_1, S_2 ...$ with Markov property. More formaly, Markov process is a tuple $\\langle \\mathcal{S}, \\mathcal{P} \\rangle$ where $\\mathcal{S}$ is set of states and $\\mathcal{P}$ is state transition matrix.\n",
    "\n",
    "<img src=\"imgs/mp.png\">\n",
    "\n",
    "<img src=\"imgs/stochastic-process-not-markov.gif\">\n",
    "<img src=\"imgs/stochastic-process-is-markov.gif\">\n",
    "\n",
    "_def_. **Markov reward process** is a Markov process with value judgements - how good it is to be in a certain stated. Markov reward process is a tuple $\\langle \\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\gamma \\rangle$ where $\\mathcal{S}$ is set of states, $\\mathcal{P}$ is state transition matrix, $\\mathcal{R}$ is reward function and $\\gamma$ is a discount factor.\n",
    "\n",
    "<img src=\"imgs/mrp.png\">\n",
    "\n",
    "\n",
    "_def_. **Return** $G_t$ is a total dicounted sum of rewards you get after time $t$.\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} ... = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "_def_. The **goal** is a optimal return. The goal of reinforcement learning is to optimize return.\n",
    "\n",
    "_def_. **Value function** $v(s)$ gives the long-term value of being in a state (expectation)\n",
    "\n",
    "<img src=\"imgs/value_functions.png\">\n",
    "\n",
    "_def_. **Bellman equation for MRPs** is following equation and it basically states that value function can be decomposed into 2 parts: 1. imidiate reward $R_{t+1}$ 2. discounted value of successor state $\\gamma v(S_{t+1})$\n",
    "\n",
    "<img src=\"imgs/bellman_mrps.png\">\n",
    "<img src=\"imgs/IMG_5525.jpg\">\n",
    "\n",
    "_def_. **Backup diagram** is one-step look ahead tree which helps us visualize one step of a process.\n",
    "\n",
    "<img src=\"imgs/calc_value_function.png\">\n",
    "\n",
    "<img src=\"imgs/bellman_eq_mat.png\">\n",
    "\n",
    "Because Bellman equation is linear it is possible to be solved (calculate $v$) directly:\n",
    "\n",
    "<img src=\"imgs/solve_bellman_eq.png\">\n",
    "\n",
    "Computational complexity of this calculation is $O(n^3)$ and therefore direct solution is applicable only to small MRPs. For larger MRPs other methods are avalible: Dynamic programming, Monte-carlo evaluation and Temporal-Difference learning.\n",
    "\n",
    "_def_. **Markov decision process**\n",
    "\n",
    "<img src=\"imgs/mdp.png\">\n",
    "\n",
    "_def_. **Policy** is a mapping from states to probabilities of selecting each possible action.\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = P(A_t=a|S_t=s)\n",
    "$$\n",
    "\n",
    "<img src=\"imgs/mdp_note.png\">\n",
    "\n",
    "_def_. Previously, in MRPs, we defined a value function $v(s)$ as a long-term value of being in a state. In MDPs we are defining **state-value function**  which tells us how good is to be in the state following the policy $\\pi$.State-value function is defined as expected return starting from $s$ following policy $\\pi$.\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = E_{\\pi}(G_t|S_t=s)\n",
    "$$\n",
    "\n",
    "_def_. **Action-value function** tells us how good is to take certain action from a peticular state while following the policy $\\pi$. We defined action-value function $q_{\\pi}(s, a)$ as a expected return when starting from state $s$, taking the action $a$ while following policy $\\pi$:\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = E_{\\pi}(G_t|S_t=s, A_t=a)\n",
    "$$\n",
    "\n",
    "\n",
    "We can now also define how Bellman equation looks like in MDPs.\n",
    "\n",
    "_def_. **Bellman expectation equation for state-value function**:\n",
    "\n",
    "<img src=\"imgs/bellman_eq_sv.png\">\n",
    "\n",
    "<!--- \n",
    "$$\n",
    "\\mathsf{v}_{\\pi}(s) = \\mathbb{E}_{\\pi}(R_{t+1} + \\gamma\\mathsf{v}_{\\pi}(S_{t+1}) | S_t = s) = \\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s, a)\n",
    "$$\n",
    " -->\n",
    "_def_. **Bellman expectation equation for action-value function**:\n",
    "<img src=\"imgs/bellman_eq_av.png\">\n",
    "<!-- \n",
    "$$\n",
    "q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}(R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a) = R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{ss'}^{a}\\mathsf{v}_{\\pi}(s')\n",
    "$$\n",
    " -->\n",
    " \n",
    "Therefore,\n",
    "<img src=\"imgs/e_bellman_eq1.png\">\n",
    "<img src=\"imgs/e_bellman_eq2.png\">\n",
    "<img src=\"imgs/e_bellman_eq3.png\">\n",
    "<img src=\"imgs/e_bellman_eq4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimality\n",
    "\n",
    "_def_. **Optimal value functions**\n",
    "<img src=\"imgs/optimal_value_functions.png\">\n",
    "\n",
    "_def_. **Optimal policy**\n",
    "<img src=\"imgs/optimal_policy.png\">\n",
    "\n",
    "_def_. **Bellman optimality equation**\n",
    "<img src=\"imgs/o_bellman_eq1.png\">\n",
    "<img src=\"imgs/o_bellman_eq2.png\">\n",
    "<img src=\"imgs/o_bellman_eq3.png\">\n",
    "<img src=\"imgs/o_bellman_eq4.png\">\n",
    "\n",
    "### Example\n",
    "<img src=\"imgs/recycling_robot_1.jpg\">\n",
    "<img src=\"imgs/recycling_robot_2.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3. Planning by Dynamic Programming\n",
    "\n",
    "* What is DP?\n",
    "* What is planning problem?\n",
    "* What is prediction and what is control?\n",
    "* What is policy evaluation?\n",
    "* Write down iterative policy evaluation algorithm?\n",
    "* Formulate and prove policy improvement theorem?\n",
    "* Write down policy iteration algorithm?\n",
    "* What is modified policy iteration?\n",
    "* What is generalized policy iteration?\n",
    "* Formulate priciple of optimality?\n",
    "* Describe value iteration algorithm?\n",
    "* Sync vs async dp?\n",
    "\n",
    "**Dynamic programming** is the method for solving complex problems by 1. breaking them down into subproblems -> 2. solve the subproblems -> 3. combine solutions to subproblems\n",
    "\n",
    "Dynamic Programming is a very general solution method for problems which have two properties:\n",
    "1. Optimal substructure\n",
    "    * Principle of optimality applies\n",
    "    * Optimal solution can be decomposed into subproblems\n",
    "2. Overlapping subproblems\n",
    "    * Subproblems recur many times\n",
    "    * Solutions can be cached and reused\n",
    "    \n",
    "MDPs satisfy both properties - Bellman equation gives recursive decomposition and value function stores and reuses solutions (value functions are *cache*).\n",
    "\n",
    "_def_. **Planning** is the problem of solving the MDP (finding the optimal policy) given the full knowledge about  MDP (we know rewards and dynamics). DP is used for solving a planning problem. Solving the planning can be split into 2 steps:\n",
    "1. **Prediction** is the problem of evaluating the policy $\\pi$\n",
    "2. **Control** is the problem of finding the optimal policy.\n",
    "\n",
    "<img src=\"imgs/prediction_and_control.png\">\n",
    "\n",
    "_def_. **Policy evaluation** is the process of figuring out how to evaluate the policy ie. if someone gives us the policy we need to figure out how much reward we are going to collect when following that policy. Policy evaluation is iterative algorithm based on Bellman expectation equation.\n",
    "\n",
    "<img src=\"imgs/policy_eval.png\">\n",
    "\n",
    "Note that value function $V$ is the 1-dim array (cache) of values of each state.\n",
    "\n",
    "<img src=\"imgs/iterative_policy_eval_bkp_diag.png\">\n",
    "\n",
    "_def_. **Policy iteration** is the process of finding the optimal policy. Policy iteration is iterative algorithm based on Bellman optimality equation. Policy iteration consist of two steps that are repeating in a cycle, policy evaluation and policy improvement.\n",
    "\n",
    "\n",
    "<img src=\"imgs/policy_iter.png\">\n",
    "\n",
    "_def_. **Policy improvement** is the process of improving the policy. We are starting from policy $\\pi$ and the way we come up with the new policy is to act greedly with respect to current action-value function. More formally,\n",
    "\n",
    "$$\n",
    "\\pi'(a|s) = \\underset{a \\in A}{\\operatorname{argmax}}q_{\\pi}(s, a)\n",
    "\\tag{1}    \n",
    "$$\n",
    "\n",
    "\n",
    "Lets explain the following theorem by focusing on a special case of *determinstic policy* $a = \\pi(s)$. The theorem is easily expandable to the case of stochastic policies $\\pi(a|s)$.\n",
    "\n",
    "\n",
    "_theorem_. (**Policy improvement theorem**) Let $\\pi$ and $\\pi'$ be any pair of deterministic policies such that, for all $s \\in S$ \n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, \\pi'(s)) \\ge v_{\\pi}(s)\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "Then the policy $\\pi'$ must be as good as, or better then, $\\pi$. That is, it must obtain greater or equal expected return from all states $s \\in S$\n",
    "\n",
    "$$\n",
    "v_{\\pi'}(s) \\ge v_{\\pi}(s).\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "$\\Delta.$ TODO\n",
    "\n",
    "This means that if we are acting greedly (like described by equation (1)) then the condition for policy improvement theorem (equation (2)) is fullfiled so we can apply the theorem and conclude that updating policy in the greedy manner will indeed improve policy. Another conclusion is that if improvement stops that means that we reached optimal policy.\n",
    "\n",
    "\n",
    "<img src=\"imgs/policy_iteration_2.png\">\n",
    "\n",
    "\n",
    "_def_. **Modified policy iteration** is essential the same algorithm as policy iteration except it can have several variations:\n",
    "1. We can introduce stopping trashold $\\epsilon$ and then not wait to find exact optimal policy until the end of the loop, but instead stop a little earilier (while ($\\pi' - \\pi) > \\epsilon$)\n",
    "2. Or simply stop after $k$ iterations of iterative policy evaluation\n",
    "\n",
    "So far, we were discussing *policy iteration* algorithm by combining *iterative policy evaluation* and *greedy policy improvement*. **Generalized policy iteration** is the same approach except we can combine **any** policy evaluation algorithm and **any** policy improvement algorithm.\n",
    "\n",
    "<img src=\"imgs/generalized_policy_iteration.png\">\n",
    "\n",
    "_theorem_. **Principle of optimality**\n",
    "<img src=imgs/principle_of_optimality.png>\n",
    "\n",
    "_def_. **Value iteration**\n",
    "\n",
    "<img src=\"imgs/deterministic_value_iter.png\">\n",
    "<img src=\"imgs/value_iter1.png\">\n",
    "<img src=\"imgs/value_iter2.png\">\n",
    "<img src=\"imgs/value_iteration.png\">\n",
    "\n",
    "<img src=\"imgs/sync_dp.png\">\n",
    "\n",
    "_def_. **Asynchronous dynamic programming** TODO\n",
    "\n",
    "TODO other ideas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 4. Model-free prediction\n",
    "\n",
    "* What is model-free prediction?\n",
    "* Describe first / every-visit Monte-carlo methods?\n",
    "* What is incremental mean?\n",
    "* Describe incremental Monte-carlo method?\n",
    "* Explain how TD methods work?\n",
    "* What is bootstrapping?\n",
    "* Describe TD(0) algorithm?\n",
    "* MC vs TD - advantages and disadvantages?\n",
    "* Batch MC / TD?\n",
    "* Describe backup diagrams of mc, td and dp?\n",
    "* Present unified view of methods for solving policy evaluation problem?\n",
    "* Explain TD($\\lambda$)?\n",
    "* Explain Forward view of TD($\\lambda$)?\n",
    "* What is eligibility trace?\n",
    "* Explain Backward view of TD($\\lambda$)?\n",
    "\n",
    "\n",
    "In this lecture we are looking at the problem of the prediction (policy evaluation) in a setting where we don't have the knowledge about MDP ie. **model-free** setting.\n",
    "\n",
    "#### Monte-Carlo methods\n",
    "\n",
    "The main idea is that since we don't know the MDP (no knowledge about dynamics and reward function) we try to solve the MDP (learn the value functions) by interacting with the enviroment (from the experience). Monte-Carlo methods learn from **complete episodes**.\n",
    "<img src=\"imgs/mc1.png\">\n",
    "\n",
    "_def_. **First-visit Monte-Carlo method**\n",
    "<img src=\"imgs/mc_first_visit.png\">\n",
    "\n",
    "_def_. **Every-visit Monte-Carlo method**\n",
    "<img src=\"imgs/mc_every_visit.png\">\n",
    "\n",
    "Note: in case of first-time MC you can update the counter only once during the episode. In case of every-visit MC you can update counter every time you enter the state.\n",
    "\n",
    "_def_. **Incremental mean**\n",
    "<img src=\"imgs/incremental_mean.png\">\n",
    "\n",
    "_def_. **Incremental Monte-Carlo**\n",
    "<img src=\"imgs/mc_incremental.png\">\n",
    "\n",
    "#### Temporal Difference Learning\n",
    "\n",
    "<img src=\"imgs/td.png\">\n",
    "\n",
    "Main difference between MC and TD is that TD algorithms learn from **incomplete episodes** by **bootstrapping**. \n",
    "*Bootstraping* is techinque of estimating the return that we will collect by the end of the episode from the state we are currently at, and then use that estimate to update state values (that is why we don't need complete episodes).\n",
    "\n",
    "<img src=\"imgs/mctd.png\">\n",
    "<img src=\"imgs/driving_home_ex.png\">\n",
    "<img src=\"imgs/driving_home_ex2.png\">\n",
    "\n",
    "#### MC vs TD\n",
    "$$\n",
    "\\pi(s|a) = \\binom{left, right}{0.5, 0.5}, \\forall s \\in \\{A, B, C, D, E\\}\n",
    "$$\n",
    "<img src=\"imgs/random_walk_ex.png\">\n",
    "<img src=\"imgs/mcvstd1.png\">\n",
    "<img src=\"imgs/mcvstd2.png\">\n",
    "<img src=\"imgs/mcvstd3.png\">\n",
    "<img src=\"imgs/mcvstd4.png\">\n",
    "\n",
    "_def_. In the case when we have only finite set of episodes ie. in case we can't run as much as runs as we want we call the algoritam **Batched**.\n",
    "\n",
    "This is how backup diagrams look like for Monte-Carlo, TD and DP.\n",
    "<img src=\"imgs/mcbackup.png\">\n",
    "<img src=\"imgs/tdbackup.png\">\n",
    "<img src=\"imgs/dpbackup.png\">\n",
    "\n",
    "#### Unified view of policy evaluation methods\n",
    "\n",
    "<img src=\"imgs/view_of_policy_eval.png\">\n",
    "\n",
    "#### TD($\\lambda$)\n",
    "\n",
    "Natural question that arises is why not take multiple steps of lookahead and then use that to do the estimate? TD(0) was the special case of 1-step lookahead.\n",
    "\n",
    "<img src=\"imgs/n_step_algo1.png\">\n",
    "<img src=\"imgs/n_step_algo2.png\">\n",
    "\n",
    "So the new question that pops up is how to choose $n$ and the answer is that we don't have to necessarly choose $n$ ... we can average over return of multiple $n$'s. That way we can combine information from all step sizes.\n",
    "\n",
    "<img src=\"imgs/n_step_algo3.png\">\n",
    "\n",
    "We don't necessarely have to do the average. We can come up with the efficient algorithm to combine all $n$'s by introducing weight (parameter $\\lambda$) to each $n$-step return. This algorithm is called **TD($\\lambda$)**.\n",
    "\n",
    "<img src=\"imgs/n_step_algo4.png\">\n",
    "\n",
    "#### Forward view of TD($\\lambda$)\n",
    "\n",
    "<img src=\"imgs/forward_view_td_lambda.png\">\n",
    "<img src=\"imgs/backward_view_td_lambda1.png\">\n",
    "\n",
    "#### Backward view of TD($\\lambda$)\n",
    "\n",
    "_def_. **Eligibility trace**\n",
    "<img src=\"imgs/eligibility_trace.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 5. Model free control\n",
    "\n",
    "1. On-policy vs off-policy learning?\n",
    "2. Greedy policy iteration with Monte-carlo model free policy evaluation...whats wrong with that approach?\n",
    "3. How we can address the problems from previous question?\n",
    "4. How we can improve eps-greedy policy iteration with monte-carlo model free policy evaluation?\n",
    "5. What is GLIE?\n",
    "6. Describe GLIE Monte-carlo control algorithm?\n",
    "7. Explain SARSA algorithm?\n",
    "8. Explain SARSA($\\lambda$) algorithm?\n",
    "9. Off-policy learning problem definition?\n",
    "10. What is importance sampling?\n",
    "11. Importance sampling with MC?\n",
    "12. Importance sampling with TD?\n",
    "13. QLearning\n",
    "14. Compare sample based methods with DP methods?\n",
    "\n",
    "\n",
    "Methods for model free control can be roughly devide into 2 categories:\n",
    "1. **on-policy learning** meaning learning on the job - learning about policy $\\pi$ from experience sampled from policy $\\pi$\n",
    "2. **off-policy learning** meaning learning from someone elses behaviour - learning about policy $\\pi$ from experience sampled from policy $\\mu$\n",
    "\n",
    "\n",
    "#### On-policy methods\n",
    "\n",
    "First thing that comes to mind about how we can solve model-free control problem is that we can use **Generalized policy iteration** framework described in lecture 3 together with some of the model-free policy evaluation method from previous lecture (monte-carlo or td). Lets see how that would work with Monte-carlo policy evaluation:\n",
    "\n",
    "<img src=\"imgs/gen_policy_iteration_with_mc_eval.png\">\n",
    "\n",
    "There are 2 problems with this approach:\n",
    "1. Greedy policy iteration requires knowledge of MDP($P_{ss'}$ transition matrix) in order to perform greedy poilcy update:\n",
    "<img src=\"imgs/model_free_policy_iter1.png\">\n",
    "2. If we act greedly with respect to $\\pi$ all the time issue of exploration arieses.\n",
    "\n",
    "In order to address the first problem we can update policy $\\pi$ by acting greedly with respect to action-value function:\n",
    "<img src=\"imgs/model_free_policy_iter2.png\">\n",
    "\n",
    "So now, we have the policy iteration with respect to *action-value* function:\n",
    "\n",
    "<img src=\"imgs/model_free_policy_iter3.png\">\n",
    "\n",
    "In order to address the second problem we can use $\\epsilon$-greedy behavior instead of greedy. $\\epsilon$-greedy behavior is explained in the lecture 1 of this doc, but here is quick remider:\n",
    "\n",
    "<img src=\"imgs/eps_greedy_policy_iter1.png\">\n",
    "<img src=\"imgs/eps_greedy_policy_iter2.png\">\n",
    "\n",
    "So now, we ended up with something like this:\n",
    "\n",
    "<img src=\"imgs/model_free_policy_iter4.png\">\n",
    "\n",
    "This would now work but we can stil optimize it a bit. Its not necessary to wait that we update action-value function for each state. Why not run a single episode, and visit some states and then update action-value function for those states only:\n",
    "\n",
    "<img src=\"imgs/eps_greedy_policy_iter3.png\">\n",
    "\n",
    "_def_. **Greedy in the limit with infinite exploration** is the behavior that has 2 properties: first that behavior guarantees that all states will be visited as number of samples approaches infinity and the second is that the behavior converges to greedy behaviour\n",
    "\n",
    "<img src=\"imgs/glie1.png\">\n",
    "\n",
    "_def_. **GLIE Monte-Carlo control algorithm**\n",
    "\n",
    "<img src=\"imgs/glie_mc_control.png\">\n",
    "\n",
    "This is our first algorithm that solves full RL problem.\n",
    "\n",
    "In previous lecture we introduced two methods for policy evaluation (MC and TD) and we saw that TD has several advantages over MC. We can now plug in TD as a policy evaluation step for control:\n",
    "<img src=\"imgs/td_control.png\">\n",
    "\n",
    "_def_. **SARSA** \n",
    "\n",
    "<img src=\"imgs/sarsa1.png\">\n",
    "<img src=\"imgs/sarsa2.png\">\n",
    "<img src=\"imgs/sarsa3.png\">\n",
    "<img src=\"imgs/sarsa4.png\">\n",
    "\n",
    "_def_. **SARSA($\\lambda$)**\n",
    "\n",
    "<img src=\"imgs/nstepsarsa.png\">\n",
    "<img src=\"imgs/forward_sarsa.png\">\n",
    "<img src=\"imgs/backward_sarsa.png\">\n",
    "<img src=\"imgs/sarsa_pseudo.png\">\n",
    "\n",
    "\n",
    "#### Off-policy methods\n",
    "\n",
    "In off-policy learning we want to evaluate policy $\\pi$ by following some other policy - behavioral policy $\\mu$. Why this approach is important?\n",
    "1. Learn from observing humans or other agents\n",
    "2. Re-use experience generated from old policies $\\pi_1 ... \\pi_{t-1}$ (used in Atari)\n",
    "3. Learn about optimal policy while following exploratory policy\n",
    "4. Learn about multiple policies while following one policy\n",
    "\n",
    "#### Importance sampling\n",
    "<img src=\"imgs/importance_sampling.png\">\n",
    "\n",
    "Importance sampling used with MC for off-policy learning doesn't work in practice because of extremly high variance.\n",
    "\n",
    "<img src=\"imgs/mc_importance_sampling.png\">\n",
    "\n",
    "It becomes imperative for using importance sampling with TD for off-policy learning. The great thing is that we can just do updates after single step by following the update rule:\n",
    "\n",
    "<img src=\"imgs/td_importance_sampling.png\">\n",
    "\n",
    "#### Q-learning\n",
    "\n",
    "\n",
    "<img src=\"imgs/qlearning.png\">\n",
    "<img src=\"imgs/qlearning1.png\">\n",
    "<img src=\"imgs/qlearning2.png\">\n",
    "<img src=\"imgs/qlearning3.png\">\n",
    "\n",
    "#### DP vs Sample based methods\n",
    "<img src=\"imgs/dp_comparison1.png\">\n",
    "<img src=\"imgs/dp_comparison2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 6. Value function approximation\n",
    "\n",
    "1. What is motivation for value function approx?\n",
    "2. Value func approx in a nutshell?\n",
    "3. 3 types of value func approx?\n",
    "4. SGD for value func approx?\n",
    "5. Defining feature vectors for RL states?\n",
    "6. How do we put things together in our algorthm when approximating value function?\n",
    "7. MC with value func approx?\n",
    "8. TD with value func approx?\n",
    "9. TD($\\lambda$) with value func approx?\n",
    "10. action-value func approx perspective?\n",
    "11. should we bootstrap when doing value func approx?\n",
    "12. batch methods\n",
    "13. experience replay\n",
    "14. dqn\n",
    "\n",
    "In methods that we seen so far we were coding the value functions as a lookup tables. The problem with that approach is that real-world problems can have extremly large state space (backgammon $10^{20}$ states ...). The goal of this lecture is to present methods that can handle infinitly large state spaces.\n",
    "\n",
    "The main idea in a nutshell is that we will try to approx value function with polinomial function with parameters $w$.\n",
    "<img src=\"imgs/value_func_aprox.png\">\n",
    "\n",
    "There are 3 types of value function approximations:\n",
    "1. State-value function approx - We are approximating state-value function\n",
    "2. Action-in action-value function approx - We are approximating action-value function by inputing both $s$ and $a$ and the result is value of how good is to take action $a$ from state $s$\n",
    "3. Action-out action-value function approx - Equvialent to 2) except we are inputing only $s$ this time and expecting values from action-value function for all possible actions from state $s$.\n",
    ".\n",
    "\n",
    "<img src=\"imgs/types_of_value_func_aprox.png\">\n",
    "\n",
    "There are many ways to approximate a function - linear function approximation, neural nets, decision trees etc. We will be focusing on neural nets.\n",
    "\n",
    "### Incremental methods\n",
    "\n",
    "#### State-value function approximation perspective\n",
    "\n",
    "Hypotetically, if we have true value of the state we could then approximate value function by minimising the mean square error between true and approximated value using SGD.\n",
    "<img src=\"imgs/value_func_aprox_sgd.png\">\n",
    "\n",
    "##### State feature vectors \n",
    "<img src=\"imgs/features_vectors.png\">\n",
    "\n",
    "Value function can be represented as a linear combination of feature vectors. Lets assume that we are given ground truth $v_{\\pi}(s)$\n",
    "\n",
    "<img src=\"imgs/value_func_approx2.png\">\n",
    "\n",
    "But in reality we dont have ground truth $v_{\\pi}(s)$ and we have to learn those from the experience. So what do we do?\n",
    "<img src=\"imgs/incremental_prediction_algorithms.png\">\n",
    "\n",
    "\n",
    "##### Monte-Carlo with value function approximation\n",
    "\n",
    "<img src=\"imgs/mc_vfn_approx.png\">\n",
    "\n",
    "##### TD with value function approximation\n",
    "\n",
    "<img src=\"imgs/td_vfn_approx.png\">\n",
    "\n",
    "##### TD($\\lambda$) with value function approximation\n",
    "\n",
    "<img src=\"imgs/td_lambda_vfn_approx.png\">\n",
    "\n",
    "\n",
    "#### Action-value function approximation perspective\n",
    "\n",
    "Everything we speak so far in this lecture was about approximating state-value function. Lets now consider doing the same thing for action value function.\n",
    "\n",
    "<img src=\"imgs/action_value_func_approx.png\">\n",
    "<img src=\"imgs/action_value_func_approx2.png\">\n",
    "\n",
    "\n",
    "#### Should we bootstrap when doing value function approximation?\n",
    "\n",
    "And the answer is yes. Following figure shows difference between mc, td and td($\\lambda$) while applying them with value function approx. We can see on the figure that MC usually blows away, td(0) is slightly better but the sweet spot is somewhere in between mc and td(0)\n",
    "\n",
    "<img src=\"imgs/should_we_bootstrap.png\">\n",
    "\n",
    "### Batch methods\n",
    "\n",
    "<img src=\"imgs/batch_methods.png\">\n",
    "<img src=\"imgs/batch_methods2.png\">\n",
    "<img src=\"imgs/experience_replay.png\">\n",
    "<img src=\"imgs/dqn.png\">\n",
    "<img src=\"imgs/dqn_net.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
