{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1. Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning\n",
    "\n",
    "We are introducing $ReLU$ (Rectified Linear Unit) function which is used as a activation function. \n",
    "\\begin{align}\n",
    "f(x) =\n",
    "\t\\begin{cases}\n",
    "\t\tx   & \\quad x \\geq 0\\\\\n",
    "\t\t0\t& \\quad x < 0\n",
    "\t\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "One of the major breakthroughs in NN computations was introducing $ReLU$ function instead of $Sigmoid$ function $f(x) = \\frac{1}{1 + e ^{-x}}$. The advantage of $ReLU$ is that has value $0$ when $x < 0$, where $Sigmoid$ function has values that are close to $0$ and thus making the computation harder.\n",
    "Additionally, gradient of $ReLU$ function is equal to $1$ for all positive $x$'s and 0 for other values and thus making the gradient descent runs much faster.\n",
    "\n",
    "Especially interesting in Week 1 of the course was interview with Geoffrey Hinton where a lot of interesting ideas were mentioned so it would be really beneficial to watch this video again once I gain more knowledge about different models like Boltzman Machines etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "x = np.arange(-3, 3, 0.1, dtype = float)\n",
    "y = relu(x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Rectified Linear Unit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation graph and chain rule\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image representation\n",
    "\n",
    "Image is represented with the 3-dimensional matrix where dimensions represent values of Red Green and Blue respectively. 3-dimensional matrix is usually squeezed into 1-dimensional vector and this is what we are considering to be our $x$ in terms of image classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification and Neural Network Notation\n",
    "In this section Andrew Ng defines binary classification problem, introduces Logistic Regression and $Sigmoid$ function but more importantly it introduces neural network notation that will be used throughout the course. He emphasizes that neural network notation will be different from the one used in his first Machine Learning course in a way that bias will be kept separately of parameters vector because it is more natural notation when implementing neural net.\n",
    "\n",
    "For detail notation take a look at the file $\\textit{neuralnetworknotation.pdf}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression cost function\n",
    "We define a \\textbf{loss function} as a measurement of how well we are doing on a single training example and \\textbf{cost function} of how well we are doing on the entire training set.\n",
    "\n",
    "Loss function for Logistic regression is:\n",
    "\n",
    "\\begin{align}\n",
    "L(y, \\hat{y}) = -(y\\log{\\hat{y}} + (1 - y)\\log{(1 - \\hat{y})})\n",
    "\\end{align}\n",
    "\n",
    "where $\\hat{y} = \\sigma(w^Tx + b)$ and $\\sigma$ is $Sigmoid$ function (in literature when we write $\\log$ it usually stands for logarithm with the base of $e$). The questions that pops up is why we simply can't use $L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2$ as a loss function? The answer is that optimization problem that we will encounter will become non-convex and thus gradient descent may converge to local optimum instead of global optimum.\n",
    "\n",
    "Cost function for Logistic regression is:\n",
    "\\begin{align}\n",
    "J(W, b) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) = -\\frac{1}{m}\\sum_{i = 1}^{m}(y^{(i)}\\log{\\hat{y}^{(i)}} + (1 - y^{(i)})\\log{(1 - \\hat{y}^{(i)})})\n",
    "\\end{align}\n",
    "\n",
    "People usually don't do random initialization for Logistic Regression because cost function is convex and thus the usual way is to simply initialize all the parameters to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting in Python\n",
    "\n",
    "General principle for the python broadcasting is that if you have $(m, n)$ matrix and $(m, 1)$ matrix you would be able to apply any of operations $+, -, *, /$ because second matrix will be automatically extended up to $(m, n)$ by repeating the column $n$ times. Similarly, if you have $(m, n)$ matrix and $(1, n)$ matrix you would be able to apply arithmetic operations because second matrix will be automatically extended up to $(m, n)$ by repeating the raw $m$ times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Avoid using rank 1 arrays, they are not column vectors nor row vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "[ 0.71820128  0.09352367  0.43305682  0.9310565   0.79726711]\n",
      "(1, 5)\n",
      "[[ 0.71820128  0.09352367  0.43305682  0.9310565   0.79726711]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.rand(5)\n",
    "\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "a = a.reshape(1, 5)\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Use assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(a.shape == (1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Logistic Regression cost function\n",
    "\n",
    "Our task is to model the probability $p(y|x)$, so we need to come up with the function, lets name it $\\hat{y}$, that will satisfy the requirements: when $y = 1$, $p(y|x) = \\hat{y}$ and when $y = 0$, $p(y|x) = 1 - \\hat{y}$. These 2 cases described $p(y|x)$ so we are now going to try to write $p(y|x)$ in a single equation: \n",
    "\n",
    "\\begin{align}\n",
    "p(y|x) = \\hat{y}^y (1 - \\hat{y})^{1 - y}\n",
    "\\end{align}\n",
    "\n",
    "Since $\\log$ is strictly monotonically increasing function, instead of modeling $p(y|x)$ we can model $\\log(p(y|x)$ and that would give us the same results. Introducing the log function will give us something like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\log{p(y|x)} = y\\log{\\hat{y}} + (1 - y)\\log{(1 - \\hat{y})}\n",
    "\\end{align}\n",
    "\n",
    "In machine learning we usually want to minimize the loss and that means maximizing the $p(y|x)$ and thus we define $L(y, \\hat{y}) = -p(y|x)$. We can now define $\\hat{y}$ as a simple polinomial function and add sigmoid to make it satisify probability requirements to be between $0$ and $1$. Thus $\\hat{y} = \\sigma(\\theta^Tx + b)$. Also, when caluclating gradients sigmoid function has a convinient property $\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$ so that is something that we should keep in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $\\frac{x}{\\| x\\|}$ (dividing each row vector of x by its norm).\n",
    "For example, if $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}$$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "Softmax or normalized exponential function is a generalization of Sigmoid function that maps vectors from $\\mathbb{R}^n$ into $n$ dimensional vector of real numbers from $[0, 1]$ range, that adds up to $1$.\n",
    "\n",
    "$$\n",
    "softmax: \\mathbb{R}^n \\rightarrow [0, 1]^n\n",
    "$$\n",
    "\n",
    "$$\n",
    "softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2  &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming assignement - cats classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b ∗ c ∗ d, a) is to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 10, 10, 3)\n",
      "(300, 200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(200, 10, 10 , 3)\n",
    "print(X.shape)\n",
    "X_flatten = X.reshape(X.shape[0], -1).T\n",
    "print(X_flatten.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel). Lets standardize our data.\n",
    "\n",
    "<!-- During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !--> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks representation\n",
    "\n",
    "Neural network with input layer, one hidden layer and output layer is also refered in literature as a \"2-layer\" neural network because people usually don't take into an account the first (input) layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Activation function that works almost always better then sigmoid function is **hyperbolic tangent** or **tanh**:\n",
    "\n",
    "$$\n",
    "\\tanh: \\mathbb{R} \\rightarrow [-1, 1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(-3, 3, 0.1, dtype = float)\n",
    "y = np.tanh(x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Hyperbolic tangent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that co-domain of this function is $[-1, 1]$ and thus it makes more sence to use sigmoid function as an activation of output layer. However, using $\\tanh$ on all hidden layers gives much better performance then using sigmoid function on all layers of the network.\n",
    "\n",
    "The downside of both $sigmoid$ and $\\tanh$ functions is that derivative of these functions is close to $0$ when $x$ is very small or when $x$ is very large, and this can slow down gradient descent.\n",
    "\n",
    "**ReLU** - goto beginning where **ReLU** is defined\n",
    "\n",
    "One disadvantage of *ReLU* function is that derivative is equal to $0$ when $x$ is negative. In practice this works just fine but people are sometimes using the **Leaky ReLU** function which is just slight modification of *ReLU*.\n",
    "\n",
    "$$\n",
    "LeakyReLU(x) = \\max(\\lambda x, x)\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is some very small scalar like for example $0.0001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd8lfX5//HXBYQkzDDCSEBQARWVGUBtrdatddRWWxegVVFbq7Z2WP3WVdv+qm21VauiWAEVB45ii7Zad+sgYchWxEGYYYSVRZLr98e5SUMIK8md+4z38/HIg3POfZ9z3vchua9zf+7rfI65OyIiIgAtog4gIiLxQ0VBRERqqCiIiEgNFQUREamhoiAiIjVUFEREpIaKgoTOzC42s3ejzrEvzOxGM3uk1vWzzWyZmW0xs6FmNt/Mjm3gY39uZic04H5bzOyAhjxnvEmmbUk2Kgqyg4busJpDUFyqgh3KJjObY2anN8HjHmtmhbVvc/ffuPtltW76PXC1u7dz91nufqi7v9nY564ny2Nmdkd9y4LnXtrUz7mvdlXk9+V3p/a27G6bpfmpKEiiec/d2wFZwF+Ap8wsqxmetw8wvxmeJ66YWauoM0jzUlGQvWZmp5vZbDMrNrP/mtmgWstuMLNPzWyzmS0ws7N38zh3mdm7ZtbRzNab2eG1lnUzs1Izy95dFnevBiYDbYH+te5/RJCtODiSOLbWss5m9lczW2FmG8zsRTNrC7wM5ARHIFvMLMfMbjWzx80s3cy2AC2BOWb2afBYNe+KzaxFre1fZ2bPmFnnWs872sy+CJbdtLevdz2vm5tZv+DyY2Z2v5n9I3jNPzCzA2ute7CZvRq8vovN7Du1ln3DzGYFR1vLzOzWWsv6Bs9zqZl9CbzewKx7yudm1s/MxgEXAj8LXvuXGvJ80nRUFGSvmNkw4FHgCqAL8BAwzczSg1U+BY4GOgK3AY+bWc86j9HCzB4GBgEnuftG4CngolqrnQ+85u5Fe8jTErgE2AZ8EdyWC/wDuAPoDPwEeK5WgZkMtAEOBboBd7v7VuBUYEUwpNHO3Vdsfx53Lw+OTAAGu3vNjq2Wa4BvAscAOcAG4P4g00DgAWB0sKwL0Gt327YPzif2WncClgC/Dp6zLfAq8GSwnecDfzGzQ4P7bQXGEDva+gZwlZl9s85jHwMcApzc1Plqc/fxwBPAncFrf0Yjnk+agIqC7K3LgYfc/QN3r3L3iUA5cASAuz/r7ivcvdrdnwY+AUbWun8aMIXYzvoMdy8Jbp8IXGBm238XRxPbee/KEWZWDJQRG+e/yN3XBMsuAqa7+/Qgx6tAPnBaUKBOBa509w3uvs3d32rUK/I/VwA3uXuhu5cDtwLnBEMv5wB/d/e3g2W/BKqb6Hmfd/cP3b2S2I51SHD76cDn7v5Xd69095nAc0EW3P1Nd58bvEYfEft/OabOY9/q7lvdvTSEfBLHNF4oe6sPMNbMfljrttbE3v1iZmOAHwN9g2XtgK611u0HDAZGunvF9hvd/QMz2wocY2Yrg/Wm7SbH++7+VTNrB0wgdnTyTK2M55pZ7XebacAbQG9gvbtv2PtN3mt9gBfMrPbOvgroTuz1Wbb9Rnffambrmuh5V9W6XELsNd+eZ1RQPLdrRVBszWwU8P+Aw4j9H6YDz9Z57GXsWiWx17WuNGJHbnvKJ3FMRUH21jLg1+6+0xCAmfUBHgaOJ3YiuMrMZgNWa7WFxIZUXjaz49x9ca1lE4m9y18FTHX3sj2FcfctZvZ94FMze9TdZwUZJ7v75fVk7Al0NrMsdy+u+3B7er49WAZ8z93/U8/zriQ2DLP9ehtiQ0hhWga85e4n7mL5k8B9wKnuXmZm97BjAYfdvyZfAvuZmXkwzXKwXd0IhvL2kaZqjiMaPpL6pJlZRq2fVsR2+lea2SiLaRucsGxP7GSvA0UAZnYJsXehO3D3KcCNwGu1TzoSewd7NrHCMGlvQ7r7OuAR4ObgpseBM8zsZDNrGWQ/1sx6uftKYieU/2Jmncwszcy+FtxvNdDFzDru7XPX8SDw66A4YmbZZnZWsGwqcLqZfdXMWgO3s+e/u5Z1Xv/W+5jn78CA4AR3WvAzwsy2F6f2xI6aysxsJHDBPj7+B8SG724I8rUlduSRT8OKwmpAn1mIEyoKUp/pQGmtn1vdPZ/YeYX7iJ1IXQJcDODuC4A/AO8R+wM/HNjpXXOw7kRiO8bXzaxvcFshMJNYYXlnH7PeQ+ycwSB3XwacRazwFBF7x/xT/vd7PprY8MYiYA1wXfD8i4iNqy+1WNdSzj5m+BOxIa9/mdlm4H1gVPDY84EfEHt3vpLYa1e4i8fZ7gZ2fP33qQPI3TcDJwHnASuIHYH9jtgwEcD3gduDrDfzv+G3vX38cmInqI8lti1LiQ2TfWf7kcM+mgAMDF77Fxtwf2lCpi/ZkXhgZo8S6wD6v6iziKQynVOQyAVHDN8ChkabREQ0fCSRMrNfAfOAu9z9s6jziKQ6DR+JiEgNHSmIiEiNhDun0LVrV+/bt2/UMUREEkpBQcFad9/tnGIQYlEwswzgbWJtcK2IfSjpljrrpBPrSx8OrAO+6+6f7+5x+/btS35+fiiZRUSSlZnt1WdIwhw+KgeOc/fBxOY8OcXMjqizzqXABnfvB9xNrJdaREQiElpR8JgtwdW04KfuWe2ziE1xALFPfh5vZoaIiEQi1BPNwVQDs4l9evRVd/+gziq5BBNvBTMpbqSeeWHMbJyZ5ZtZflHRbmdUFhGRRgi1KARTLA8hNn/8SDOrOx9OfUcFO/XIuvt4d89z97zs7D2eJxERkQZqlpbUYFbKN4FT6iwqJDal8fav/esIrG+OTCIisrPQikIwU2RWcDkTOIHYRGS1TQPGBpfPAV5v4IRaIiLSBML8nEJPYGLwtYktgGfc/e9mdjuQ7+7TiM2OONnMlhA7QjgvxDwiIrIHoRWF4Gv+dprgzN1vrnW5DDg3rAwiIsniT699wokDuzMwp0Ooz5Nwn2gWEUk1k9/7nLtf+5jyyqrQi4LmPhIRiWPvfrKWW19awPEHd+P6kw4K/flUFERE4tSnRVv4/hMF9Mtuxz3nDaFli/A/26uiICISh4pLKrhsYj5pLVvwyNg82mekNcvz6pyCiEic2VZVzQ+enMnyDaU8cfkoendu02zPraIgIhJH3J3bXprPf5as4/fnDmZE387N+vwaPhIRiSOT3vuCx9//kiuOOYBzhvdq9udXURARiRNvf1zEbS/N54RDuvOzkw+OJIOKgohIHFiyZgs/eHImA7q3b7ZOo/qoKIiIRGzD1gounTiD9FaxTqN26dGd7tWJZhGRCFVUVnPVEwWsLC5jyrgj6NWp+TqN6qOiICISEXfnlmnzeX/pev74ncEM79Mp6kgaPhIRicpf//M5Uz78ku8feyDfGtb8nUb1UVEQEYnAm4vXcMc/FnDSwO78pBnmNNpbKgoiIs3sk9Wb+eGTszi4Rwfu/u4QWkTUaVQfFQURkWa0fmsFl07MJz2tJY+MzaNthJ1G9YmvNCIiSayisporHy9g1aYynhp3BDlZmVFH2omOFEREmoG788sX5/HhZ+u565xBDNsv+k6j+qgoiIg0gwnvfsbT+cv44XH9OGtIbtRxdklFQUQkZK8vWs1vpi/k1MN68KMTBkQdZ7dUFEREQrR41WaumTKbgTkd+MN3BsdVp1F9VBREREKybks5l06cQZvWLXl4TB5tWsd/b09oRcHMepvZG2a20Mzmm9m19axzrJltNLPZwc/NYeUREWlO5ZVVXPl4AUWbyxk/Jo+eHeOv06g+YZatSuB6d59pZu2BAjN71d0X1FnvHXc/PcQcIiLNyt256YV5zPh8A/eeP5QhvbOijrTXQjtScPeV7j4zuLwZWAjE7yl3EZEm8vA7S5laUMi1x/fnjME5UcfZJ81yTsHM+gJDgQ/qWXykmc0xs5fN7NBd3H+cmeWbWX5RUVGISUVEGue1Bav57cuL+MbhPbn2+P5Rx9lnoRcFM2sHPAdc5+6b6iyeCfRx98HAvcCL9T2Gu4939zx3z8vOzg43sIhIAy1cuYlrn5rF4bkd+f258d9pVJ9Qi4KZpRErCE+4+/N1l7v7JnffElyeDqSZWdcwM4mIhGHtlnIum5hPu4xWjB+dR2brllFHapAwu48MmAAsdPc/7mKdHsF6mNnIIM+6sDKJiIShvLKKKyYXsG5rOQ+PyaNHx4yoIzVYmN1HXwFGA3PNbHZw243AfgDu/iBwDnCVmVUCpcB57u4hZhIRaVLuzi+en0vBFxu4/4JhDOqVOJ1G9QmtKLj7u8BuB9Tc/T7gvrAyiIiE7cG3lvL8zOX8+MQBfGNQz6jjNJo+0Swi0kD/nL+KO/+5iDMG5/DD4/pFHadJqCiIiDTAghWb+NHTsxmU25G7zhlEcHo04akoiIjsozWby7hs4gw6ZKTx8Jg8MtISs9OoPvE/O5OISBwp2xbrNFpfUsHUK4+iW4fE7TSqj4qCiMhecndueO4jZn1ZzAMXDuOw3I5RR2pyGj4SEdlLf3nzU16cvYKfnDSAUw9P/E6j+qgoiIjshVfmreSufy7mrCE5/ODrydFpVB8VBRGRPZi3fCM/enoOQ3pn8btvJ0+nUX1UFEREdmPNpjIun5RPpzZpjB8zPKk6jeqjE80iIrtQtq2KyycXUFyyjalXHUm39snVaVQfFQURkXq4Oz+d+hFzlhXz0OjhHJqTfJ1G9dHwkYhIPe59fQkvzVnBz045iJMP7RF1nGajoiAiUsf0uSv546sf862huVx1zIFRx2lWKgoiIrXMLdzIj5+ZzfA+nfjttw9P6k6j+qgoiIgEVm8q47JJM+jSNp2HRg8nvVVydxrVR0VBRAQoraji8kn5bCmr5JGxeXRtlx51pEio+0hEUp6785Opc5i7fCMPj87jkJ4doo4UGR0piEjK+9O/P+EfH63kF6cezAkDu0cdJ1IqCiKS0v7+0Qruee0Tzh3ei8uPPiDqOJFTURCRlDVnWTHXPzOHEX07ccfZh6Vcp1F9VBREJCWt2hib0yi7fToPXpSanUb1UVEQkZRTWlHFZZNmsLW8kgljR9AlRTuN6hNaUTCz3mb2hpktNLP5ZnZtPeuYmf3ZzJaY2UdmNiysPCIiANXVzvXPzmb+ik3ce8FQDurRPupIcSXMltRK4Hp3n2lm7YECM3vV3RfUWudUoH/wMwp4IPhXRCQU97z2MdPnruKm0w7huINTu9OoPqEdKbj7SnefGVzeDCwEcuusdhYwyWPeB7LMLDm/405EIve32cv58+tL+E5eLy47ev+o48SlZjmnYGZ9gaHAB3UW5QLLal0vZOfCgZmNM7N8M8svKioKK6aIJLFZX27gp1M/YmTfztzxzdSb02hvhV4UzKwd8Bxwnbtvqru4nrv4Tje4j3f3PHfPy87ODiOmiCSxFcWljJtcQPcO6Tw4ejitW6nHZldCnebCzNKIFYQn3P35elYpBHrXut4LWBFmJhFJLSUVlVw2MZ+yiiqevGwUndu2jjpSXAuz+8iACcBCd//jLlabBowJupCOADa6+8qwMolIaqmudn709GwWrdrEny8YSv/u6jTakzCPFL4CjAbmmtns4LYbgf0A3P1BYDpwGrAEKAEuCTGPiKSYP7y6mH/OX80vTx/I1w/qFnWchBBaUXD3d6n/nEHtdRz4QVgZRCR1vTCrkPvf+JTzR/bme1/pG3WchKGzLSKSdAq+2MDPp87liAM6c9uZmtNoX6goiEhSKdxQwhWT8+mZlcEDF6rTaF/pS3ZEJGlsLY91GpVvq+apcXl0UqfRPlNREJGkUF3tXPf0bD5evZm/XjKSft3UadQQOq4SkaRw178W8+qC1dx8+kCOGaAPuTaUioKIJLznCgp54M1PuWDUfow9qm/UcRKaioKIJLSCL9bzi+fncuQBXbjtzEPVadRIKgoikrAKN5QwblIBOVkZPHDRMNJaapfWWHoFRSQhbQk6jSqqqplw8Qiy2qjTqCmo+0hEEk5VtXPtlFl8smYLj10yggOz20UdKWnoSEFEEs6dryzi34vWcMsZAzm6vzqNmpKKgogklGfyl/HQ20sZfUQfxhzZN+o4SUdFQUQSxoefreemF+by1X5dufmMgVHHSUoqCiKSEJatL+HKxwvo3akN91+gTqOw6FUVkbi3uWwbl06cQVW1M+HiEXRskxZ1pKSloiAica2q2rlmyiyWFm3lgQuHsX/XtlFHSmpqSRWRuPbb6Qt5Y3ERvz77MI7q1zXqOElPRwoiEreenvElj7z7GRcf1ZcLR/WJOk5KUFEQkbj0/tJ1/N+L8/jagGz+7xuHRB0nZagoiEjc+WLdVq56vID9Orfh3vOH0kqdRs1Gr7SIxJVNZdu4dGI+DkwYO4KOmeo0ak4qCiISNyqrqvnhk7P4fO1WHrhwOH3VadTsQisKZvaoma0xs3m7WH6smW00s9nBz81hZRGRxPCb6Yt46+MifvXNwzjywC5Rx0lJYbakPgbcB0zazTrvuPvpIWYQkQTx5Adf8uh/PuN7X9mf80fuF3WclBXakYK7vw2sD+vxRSR5/PfTtdz8t3kcMyCbG087OOo4KS3qcwpHmtkcM3vZzA7d1UpmNs7M8s0sv6ioqDnziUjIPlu7lasen0nfrm259wJ1GkUtyld/JtDH3QcD9wIv7mpFdx/v7nnunpedrbnTRZLFxtLYnEYtDCaMzaNDhjqNohZZUXD3Te6+Jbg8HUgzM32GXSRFVFZVc/WTM1m2voQHLxpOny7qNIoHkRUFM+thZhZcHhlkWRdVHhFpXnf8YyHvfLKWO755GKMOUKdRvAit+8jMpgDHAl3NrBC4BUgDcPcHgXOAq8ysEigFznN3DyuPiMSPye9/wWP//ZzLj96f745Qp1E8Ca0ouPv5e1h+H7GWVRFJIf9ZspZbp83nuIO7ccOpmtMo3ug0v4g0m6VFW7jq8QIOzG7Ln84bQssWFnUkqUNFQUSaxcaS2JxGrVq2YMLYEbRXp1FcUlEQkdBtq6rm+08WULihhIdGD6d35zZRR5Jd0DeviUjobn9pAf9Zso67zhnEiL6do44ju6EjBREJ1aT3Pmfy+19wxdcO4Ny83lHHkT3YY1Ews6vNrFNzhBGR5PLOJ0Xc9tICTjikGz87RXMaJYK9OVLoAcwws2fM7JTtHzgTEdmdJWu28P0nZtK/WzvuOW+oOo0SxB6Lgrv/H9AfmABcDHxiZr8xswNDziYiCaq4pILLJs4gvVULHhmbR7t0nb5MFHt1TiH4pPGq4KcS6ARMNbM7Q8wmIgloW1U1Vz0+kxXFZTw0eji9OqnTKJHssXyb2TXAWGAt8AjwU3ffZmYtgE+An4UbUUQShbtz89/m897Sdfzh3MEM76NOo0SzN8d0XYFvufsXtW9092oz07emiUiNx/77OVM+/JKrjj2Qbw/vFXUcaYA9FgV33+V3J7v7wqaNIyKJ6q2Pi/jV3xdw0sDu/PSkg6KOIw2kzymISKMtWbOZq5+YyYDu7bn7u0NooU6jhKWiICKNsmFrBd97LJ/0tBZMuHgEbdVplND0vyciDVZRWc2VjxewalMZUy4/gtyszKgjSSPpSEFEGsTd+eWL8/jgs/Xc+e1BDO+jiQ+SgYqCiDTIhHc/4+n8ZVz99X58c2hu1HGkiagoiMg+e2PRGn4zfSGnHNqDH584IOo40oRUFERkn3y8ejM/nDKLQ3p24I/fHaxOoySjoiAie23dlnIunTiDzNYteWRsHm1aq1cl2agoiMheqaiMzWm0ZlM5D4/Jo2dHdRolo9CKgpk9amZrzGzeLpabmf3ZzJaY2UdmNiysLCLSOO7OTS/M5cPP13PXuYMZ0jsr6kgSkjCPFB4DTtnN8lOJTcndHxgHPBBiFhFphEfe+YxnCwq55rh+nDk4J+o4EqLQioK7vw2s380qZwGTPOZ9IMvMeoaVR0Qa5t8LV/Oblxdy2uE9uO4EdRoluyjPKeQCy2pdLwxuE5E4sXjVZq6ZMotDczrwh3M1p1EqiLIo1Pfb5fWuaDbOzPLNLL+oqCjkWCICsDboNGqb3oqHx+SR2bpl1JGkGURZFAqB3rWu9wJW1Leiu4939zx3z8vOzm6WcCKprLyyiisnF1C0WZ1GqSbKojANGBN0IR0BbHT3lRHmERFinUa/eH4u+V9s4A/fGcxgdRqllNA+eWJmU4Bjga5mVgjcAqQBuPuDwHTgNGAJUAJcElYWEdl7D729lOdnLue6E/pz+iB1GqWa0IqCu5+/h+UO/CCs5xeRfffqgtX87pVFnD6oJ9ce3z/qOBIBfaJZRABYuHIT1z41i0G5Hfn9uYMxU6dRKlJREBGKNpdz2cR8OmSkMX5MHhlp6jRKVZrNSiTFlW2r4orJ+azbWs7UK4+ie4eMqCNJhFQURFLY9k6jmV8W88CFwzgst2PUkSRiGj4SSWEPvPUpL8xazvUnDuDUwzXLjKgoiKSsV+at4s5XFnPm4ByuPq5f1HEkTqgoiKSg+Ss28qOnZzOkdxZ3njNInUZSQ0VBJMWs2VzGZRPzyWqTxvgxw9VpJDvQiWaRFFK2rYpxkwooLtnG1KuOpFt7dRrJjlQURFKEu/Pz5z5i9rJiHrxoOIfmqNNIdqbhI5EUcf8bS/jb7BX89OSDOOWwHlHHkTiloiCSAl6eu5Lf/+tjzh6ay/ePPTDqOBLHVBREkty85Rv50TOzGbpfFr/91uHqNJLdUlEQSWJrNsU6jTq3ac1Do9VpJHumE80iSapsWxWXT8pnU9k2pl55lDqNZK+oKIgkIXfnJ8/O4aPlGxk/Oo+BOR2ijiQJQsNHIknoz/9ewt8/WsnPTj6YEwd2jzqOJBAVBZEk8/ePVnD3ax/zrWG5XHnMAVHHkQSjoiCSRD4qLOb6Z+aQ16eTOo2kQVQURJLEqo2xTqOu7dJ5cPRw0lup00j2nYqCSBIorYh1Gm0tr2TCxXl0bZcedSRJUOo+Eklw1dXO9c/OZt6KjTwyJo+De6jTSBou1CMFMzvFzBab2RIzu6Ge5RebWZGZzQ5+Lgszj0gyuuffnzB97ip+cerBHH+IOo2kcUI7UjCzlsD9wIlAITDDzKa5+4I6qz7t7leHlUMkmU2bs4I///sTzh3ei8uPVqeRNF6YRwojgSXuvtTdK4CngLNCfD6RlDJ7WTE/fXYOI/t25o6zD1OnkTSJMItCLrCs1vXC4La6vm1mH5nZVDPrXd8Dmdk4M8s3s/yioqIwsooklJUbS7l8Uj7dOqTzwEXD1GkkTSbMolDf2xavc/0loK+7DwJeAybW90DuPt7d89w9Lzs7u4ljiiSWkopKLpuYT2lFFRPGjqCLOo2kCYVZFAqB2u/8ewEraq/g7uvcvTy4+jAwPMQ8Igmvutr58dNzWLhyE/eeP5QB3dtHHUmSTJhFYQbQ38z2N7PWwHnAtNormFnPWlfPBBaGmEck4d392se8Mn8VN552CF8/uFvUcSQJhdZ95O6VZnY18E+gJfCou883s9uBfHefBlxjZmcClcB64OKw8ogkur/NXs69ry/hvBG9ufSr+0cdR5KUudcd5o9veXl5np+fH3UMkWY188sNnDf+fYb2zmLypaNo3UqTEci+MbMCd8/b03r6zRKJc8uLSxk3qYAeHTJ44KLhKggSKk1zIRLHtpbHOo3Kt1Ux5fJRdG7bOupIkuRUFETiVHW186OnZ7N41SYevXgE/dVpJM1Ax6Eicer3/1rMvxas5penD+TYg9RpJM1DRUEkDj0/s5C/vPkpF4zaj4uP6ht1HEkhKgoicabgi/Xc8NxcjjqwC7edeajmNJJmpaIgEkcKN5QwblIBOVkZ/OXCYaS11J+oNC/9xonEiS1Bp1FFVTWPjB1BVht1GknzU/eRSByoqnaue2o2n6zZwmOXjKBft3ZRR5IUpSMFkThw5z8X8drC1dx8+kCO7q+ZgCU6KgoiEXs2fxkPvbWUi47YjzFH9ok6jqQ4FQWRCM34fD03vjCXr/Trwi1nqNNIoqeiIBKRZetLuGJyAb06teEvFwxXp5HEBf0WikRge6dRZVU1j4zNo2ObtKgjiQDqPhJpdlXVzrVTZrGkaAsTLxnJgdnqNJL4oSMFkWb2u1cW8e9Fa7j1zEP5av+uUccR2YGKgkgzembGMsa/vZSxR/Zh9BHqNJL4o6Ig0kw+WLqOm16cy9H9u/LL0wdGHUekXioKIs3gy3UlXPl4Ab07t+G+C4bRSp1GEqf0mykSsk1l27h04gyqHSaMHUHHTHUaSfxSURAJUWVVNT98chafrd3KAxcNY/+ubaOOJLJbakkVCdFvpi/irY+L+PXZh3HUgeo0kvgX6pGCmZ1iZovNbImZ3VDP8nQzezpY/oGZ9Q0zj0hzmvLhlzz6n8+45Ct9uXCUOo0kMYRWFMysJXA/cCowEDjfzOq2XFwKbHD3fsDdwO/CyiPSnN77dB2/fHEeXxuQzU2nHRJ1HJG9FuaRwkhgibsvdfcK4CngrDrrnAVMDC5PBY43zQgmCWpz2TYWr9rMK/NWcdUTBfTt2pb7LhiqTiNJKGGeU8gFltW6XgiM2tU67l5pZhuBLsDa2iuZ2ThgHMB+++0XVl6RXaqudtZuLWf5hlKWF5eyorg0uFzG8uJSlm8oYVNZZc36ndu2ZsLYPDpkqNNIEkuYRaG+d/zegHVw9/HAeIC8vLydlos0VnllFas2lrF8QymFO+z0Y5dXFJdRUVW9w33aZ7QiNyuT3KxMRvTtRG5WJjlZmeR2yqR/t3a0V0GQBBRmUSgEete63gtYsYt1Cs2sFdARWB9iJklRG0u37bSjLwyurygupWhLOV7r7YYZdGufTk5WJoflduTkw3rUFIDcTrGdv44CJBmFWRRmAP3NbH9gOXAecEGddaYBY4H3gHOA191dRwKyT6qrnaIt5cEwTt3hndi/m8srd7hP65YtyMnKILdTJscMyCa3U+YOO/2eHTNp3UrnAiT1hFYUgnMEVwP/BFoCj7r7fDO7Hch392nABGCymS0hdoRwXlh5JHGVbati5caymh193eGdlRtL2Va143uJDhmtyMnKpFenTEbt37nm3f2YKDiEAAAHUElEQVT2nX7Xtum0aKGeBpG6LNHemOfl5Xl+fn7UMaSJuDubSispLC5hRXEZyzeUBO/0y2qGd9ZuKd/hPmbQvX3Gjjv64F1/blYbcrIyNJ4vUoeZFbh73p7W0yeaJVRV1c6azWX/G8qpNY6/fee/pc7QTnqrFjUnbY8/uFvN0M72d/7dO2RoaEckJCoK0ihl26pq7eD/N7yzvQis2lhGZfWOR6MdM9PIzcqkT5e2HHVgV3rVGdrp0ra1vsBeJCIqCrJL7s7G0m0U1nPydnshWLulYof7tDDo3iGD3KxMhvfptMPOfvu7/Xbp+rUTiVf660xhlVXVrNm8Y9dO3eGdkoqqHe6TkdaiZkc/MKcDOR133OH36JhBmj7BK5KwVBSSWGlFVb07+pqhnU1lVNUZ2unctjU5WRkckN2Wo/tnk5OVQa9aJ3A7a2hHJKmpKCQod2dDybZgB18Sm26h5nLsBO76rTsO7bRsYfQIhnZG7t95h0/gxi5n0Ka1fiVEUpn2AHGqsqqaVZvKYm2axSU7zLOzfWy/dNuOQzuZaS2DD2S14fDcrOAEbga5WW3I7ZRJ9/bpmpxNRHZLRSEiJRWVu23TrG9op0vb1uR2yqRfdjuOGZBdM7bfK3inn9UmTUM7ItIoKgohcHfWba3YYUdfuMNOv5QNJdt2uE+rFkaPjhnkZP3vE7jbh3e27/wzW7eMaItEJFWoKDTAtqpqVm0s22FHv3xDKSs2/u8kbnnljjNqtm3dsmZHP6R31k5z7XRrn0FLTbsgIhFTUajH1vLK3bZprt5URp2RHbq2Syc3K4ODe7bnuFqfws3tlEmvrDZ0yGyloR0RiXspVxTcnbVbKnZu06xVBDaW7jy00zMr1rVz1IFdd5pnJycrk4w0De2ISOJLmaLwxuI13P7SApYXl1JRZ2inXXqrmnf1NZ/CrTW8k90+XUM7IpISUqYodGrTmoE5HThxYPeanf32nX/HTM2oKSICKVQUhvTO4v4LhkUdQ0QkrumTTCIiUkNFQUREaqgoiIhIDRUFERGpoaIgIiI1VBRERKSGioKIiNRQURARkRrm7nteK46YWRHwRQPv3hVY24RxoqbtiV/JtC2QXNuTTNsCe789fdw9e08rJVxRaAwzy3f3vKhzNBVtT/xKpm2B5NqeZNoWaPrt0fCRiIjUUFEQEZEaqVYUxkcdoIlpe+JXMm0LJNf2JNO2QBNvT0qdUxARkd1LtSMFERHZDRUFERGpkXJFwcx+ZWYfmdlsM/uXmeVEnakxzOwuM1sUbNMLZpYVdaaGMrNzzWy+mVWbWcK2DJrZKWa22MyWmNkNUedpDDN71MzWmNm8qLM0lpn1NrM3zGxh8Ht2bdSZGsPMMszsQzObE2zPbU3yuKl2TsHMOrj7puDyNcBAd78y4lgNZmYnAa+7e6WZ/Q7A3X8ecawGMbNDgGrgIeAn7p4fcaR9ZmYtgY+BE4FCYAZwvrsviDRYA5nZ14AtwCR3PyzqPI1hZj2Bnu4+08zaAwXANxP4/8aAtu6+xczSgHeBa939/cY8bsodKWwvCIG2QEJXRXf/l7tXBlffB3pFmacx3H2huy+OOkcjjQSWuPtSd68AngLOijhTg7n728D6qHM0BXdf6e4zg8ubgYVAbrSpGs5jtgRX04KfRu/PUq4oAJjZr81sGXAhcHPUeZrQ94CXow6R4nKBZbWuF5LAO55kZWZ9gaHAB9EmaRwza2lms4E1wKvu3ujtScqiYGavmdm8en7OAnD3m9y9N/AEcHW0afdsT9sTrHMTUElsm+LW3mxLgrN6bkvoo9FkY2btgOeA6+qMHCQcd69y9yHERghGmlmjh/haNT5W/HH3E/Zy1SeBfwC3hBin0fa0PWY2FjgdON7j/CTRPvzfJKpCoHet672AFRFlkTqCsffngCfc/fmo8zQVdy82szeBU4BGNQUk5ZHC7phZ/1pXzwQWRZWlKZjZKcDPgTPdvSTqPMIMoL+Z7W9mrYHzgGkRZxJqTsxOABa6+x+jztNYZpa9vdvQzDKBE2iC/Vkqdh89BxxErMvlC+BKd18ebaqGM7MlQDqwLrjp/UTtpjKzs4F7gWygGJjt7idHm2rfmdlpwD1AS+BRd/91xJEazMymAMcSm555NXCLu0+INFQDmdlXgXeAucT+/gFudPfp0aVqODMbBEwk9nvWAnjG3W9v9OOmWlEQEZFdS7nhIxER2TUVBRERqaGiICIiNVQURESkhoqCiIjUUFEQEZEaKgoiIlJDRUGkkcxsRPB9Fhlm1jaY2z6hp5mW1KUPr4k0ATO7A8gAMoFCd/9txJFEGkRFQaQJBPMczQDKgKPcvSriSCINouEjkabRGWgHtCd2xCCSkHSkINIEzGwasW9Z25/YVz7G/fd0iNQnKb9PQaQ5mdkYoNLdnwy+o/m/Znacu78edTaRfaUjBRERqaFzCiIiUkNFQUREaqgoiIhIDRUFERGpoaIgIiI1VBRERKSGioKIiNT4/85FiYAD2+alAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def leaky_relu(x, param):\n",
    "    return np.maximum(param * x, x)\n",
    "\n",
    "\n",
    "x = np.arange(-3, 3, 0.1, dtype = float)\n",
    "param = 0.1\n",
    "y = leaky_relu(x, param)\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Leaky Rectified Linear Unit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some rules that can help you choose activation function:\n",
    "* If we are solving binary classification problem then the *sigmoid* function is the natural choise for the output layer\n",
    "* *Sigmoid* function should be almost never used on a hidden layers because *tanh* is explicitly superior\n",
    "* The *ReLU* is conisdered to be the default function for the units in hidden layers and it gave the best results so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need non-linear activation functions?\n",
    "\n",
    "It could be easily proven that if you take deep neural network with linear activation functions on all layers and with *sigmoid* activation on the output layer the model will be as expresive as simple logistic regression because *the composition of two linear function is also linear function*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives of activation functions\n",
    "\n",
    "Sigmoid:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}} \\\\\n",
    "\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "Hyperbolic tangent:\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \\\\\n",
    "\\tanh'(x) = 1 - \\tanh^2(x)\n",
    "$$\n",
    "\n",
    "ReLU:\n",
    "\n",
    "$$\n",
    "ReLU(x) = \\max(0, x) \\\\\n",
    "ReLU'(x) = \\begin{cases}\n",
    "            1   & \\quad x > 0 \\\\\n",
    "            0\t& \\quad x < 0 \\\\\n",
    "   undefinded   & \\quad x = 0\n",
    "\t\\end{cases}\n",
    "$$\n",
    "\n",
    "Leaky ReLU:\n",
    "\n",
    "$$\n",
    "LeakyReLU(x) = \\max(0.0001x, x) \\\\\n",
    "LeakyReLU'(x) = \\begin{cases}\n",
    "            1   & \\quad x > 0 \\\\\n",
    "            0.0001\t& \\quad x < 0 \\\\\n",
    "       undefinded   & \\quad x = 0\n",
    "\t\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random initialization\n",
    "\n",
    "For neural nets initializing parameters to $0$ won't work. Instead, the best practice is to initialize them randomly, usually like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0015674   0.00778366]\n",
      " [ 0.00311668  0.00011872]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = np.random.rand(2, 2) * 0.01\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview with Ian Goodfellow\n",
    "\n",
    "He had 3 very important points imo:\n",
    "\n",
    "1) While learning new stuff make sure you work on a project related to the topic in paralel so that you can practice what you have learn. Also, make sure that code for the project is publicly available (github).\n",
    "\n",
    "2) A lot of the time, it's harder to reach the point where you have something polished enough to really be a new academic contribution to the scientific literature, but you can often get to the point of having a useful software product much earlier.\n",
    "\n",
    "3) Research topic\n",
    "\n",
    "In the past, we've seen computer security issues where attackers could fool a computer into running the wrong code. That's called application-level security. And there's been attacks where people can fool a computer into believing that messages on a network come from somebody that is not actually who they say they are. That's called network-level security. Now, we're starting to see that you can also fool machine-learning algorithms into doing things they shouldn't, even if the program running the machine-learning algorithm is running the correct code, even if the program running the machine-learning algorithm knows who all the messages on the network really came from. And I think, it's important to build security into a new technology near the start of its development. We found that it's very hard to build a working system first and then add security later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week is about already familiar topics such as forward and back prop and vectorization in the implementation itself. Because of that I will just write down couple of intersting points that appear in this week's lecutres:\n",
    "\n",
    "* Circut theory: there are functions you can compute with small L-layer deep network that shallower networks requires exponentially more hidden units to compute.\n",
    "\n",
    "* This graph nicely explains forward and backward propagation with caching of vector $z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/forwbackprop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parameters of neural nets are: $W, b$ etc.\n",
    "* Hyperparameters of neural nets are: learning rate, number of iterations, number of hidden layers, number of units in each layer, choise of activation function ... and advanced ones like momentum, mini batch size, regularization parameters etc\n",
    "* When debuging neural nets it is extremely important to draw this graph and visualize how your net is behaving for different learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/cost.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* At the moment there is no easy way to tell which hyperparameters will work well and which will not except to try them out and emipiricaly come to the conclusion. This might change in the future so this can also be a research topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
