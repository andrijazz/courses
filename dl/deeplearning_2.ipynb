{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical aspects of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Dev / Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of hyperparameters:\n",
    "* Number of layers\n",
    "* Number of hidden units in each layer\n",
    "* Learning rates\n",
    "* Regularization parameters\n",
    "* Activation functions\n",
    "\n",
    "In order to figure out what are the most suitable hyperparameter values for your neural net it is advised that you constantly experiment with different values and iterate on repeating the experiments.\n",
    "\n",
    "Previous era of machine learning was characterized by the fact that we had much smaller amounts of data. \n",
    "* 70 / 30 train / test split or 60 / 20 / 20 train / dev / test split was ok when you have up to 10000 examples in your dataset, but when you have 1m examples 98 / 1 / 1 is also good choise\n",
    "* Make sure that dev and test set come from the same distribution\n",
    "* Not having the test set might be ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Variance\n",
    "\n",
    "<img src=\"imgs/biasvariance.png\">\n",
    "\n",
    "It is possible that classifer has both high bias and high variance and that would look something like this:\n",
    "\n",
    "<img src=\"imgs/highbias&highvariance.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic recipe for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Try bigger network\n",
      "* Train longer\n",
      "* Search different nn architecture\n",
      "* Find more data or create more features\n",
      "* Regularization\n",
      "* Search different nn architecture\n"
     ]
    }
   ],
   "source": [
    "high_bias = True # measure error on train set \n",
    "high_variance = True # measure error on dev / test set\n",
    "\n",
    "if high_bias:\n",
    "    print(\"* Try bigger network\")\n",
    "    print(\"* Train longer\")\n",
    "    print(\"* Search different nn architecture\")\n",
    "\n",
    "if high_variance:\n",
    "    print(\"* Find more data or create more features\")\n",
    "    print(\"* Regularization\")\n",
    "    print(\"* Search different nn architecture\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Let introduce regularization on an example of Logistic Regression.\n",
    "\n",
    "L2 regularization:\n",
    "\\begin{align}\n",
    "J(W, b) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{2m}\\sum_{j = 1}^{n_x} {\\lVert w_j \\rVert}^2\n",
    "\\end{align}\n",
    "\n",
    "L1 regularization:\n",
    "\\begin{align}\n",
    "J(W, b) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{m}\\sum_{j = 1}^{n_x} {\\lVert w_j \\rVert}\n",
    "\\end{align}\n",
    "\n",
    "In general, L2 regularization is used much much more often. L1 regularization is used when we want to make $W$ sparse.\n",
    "\n",
    "In the case of Neural Networks L2 regularization would look like this:\n",
    "\\begin{align}\n",
    "J(W_1, b_1, W_2, b_2 ... W_L, b_L) = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{2m}\\sum_{l = 1}^{L} {\\lVert W_l \\rVert}^2 = \\frac{1}{m}\\sum_{i = 1}^{m}L(y^{(i)}, \\hat{y}^{(i)}) + \\frac{1}{2m}\\sum_{i = 1}^{n_{l - 1}}\\sum_{j = 1}^{n_l} {\\lVert w_{ij} \\rVert}^2\n",
    "\\end{align}\n",
    "\n",
    "because $W$'s are always $n_{l-1}$ x $n_l$ dimension.\n",
    "\n",
    "Note that L2 norm is also called Frobenius norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Regularization Reduces Overfitting\n",
    "\n",
    "Couple of good points .. watch it again\n",
    "https://www.youtube.com/watch?v=NyG-7nRpsW8&index=5&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a efficient way of regularizing neural network paramenters.\n",
    "\n",
    "Let say this is our neural network and what we are going to do is that for each layer we are going to set the probability of keeping (eliminating) each node in that layer. Then, when we do training, after one training example we decide to remove certain nodes (by removing all in-going and out-going links of that node) so we end up with much smaller network. After we do backpropagation new training instance comes along and then we choose different nodes to be eliminated and so on.\n",
    "\n",
    "<img src=\"imgs/dropout1.png\">\n",
    "<img src=\"imgs/dropout2.png\">\n",
    "\n",
    "The parameter that represents probability of keeping each node is called `keep-prob`. There are several ways to implement dropout regularization but the most common one is `Inverted dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]]\n",
      "[[ 0.17275606]\n",
      " [ 1.06163271]\n",
      " [ 0.        ]\n",
      " [ 0.49236518]]\n"
     ]
    }
   ],
   "source": [
    "# inverted dropout for layer 3, keepProb = 0.8\n",
    "import numpy as np\n",
    "keepProb = 0.8\n",
    "a3 = np.random.rand(4, 1)\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keepProb\n",
    "a3 = np.multiply(a3, d3) # element-wise multiplication\n",
    "a3 /= 0.8\n",
    "# z4 = w4 * a3 + b4\n",
    "print(d3)\n",
    "print(a3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't use dropout when making a prediction. More info and the math behind dropout can be found it Hintons paper http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout works really well in computer vision problems and it is used almost as a default because you almost never have enough data and model is usually overfitting. In other areas of ai is not used that frequently. One downside of dropout is that cost function J may not be monotonically decreasing. Because of that prefered way of debbuging the code is to plot cost function J without dropout (keep-prob is set to 1) and if everything is ok, switch on dropout by setting keep-prob to `0.8` or some other value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other regularization methods include:\n",
    "1. Data augmentation - flipping horizontally, random cropping, adding distorion transformations etc.\n",
    "2. Early stopping - During the training we usually plot train loss and val loss and what we can sometimes encounter is that at some point val loss starts to grow comparing to train loss. At this point we may want to stop the training because that will probably give us best results. Early stopping is a technique for stopping the model training at this point instead of continuing training.\n",
    "\n",
    "<img src=\"imgs/earlystopping.png\">\n",
    "\n",
    "3. Orthogonalization - is a concept that its good to keep in mind. The idea is that we treat training of the model as a 2 separate tasks. One is finding the right set of parametars to make sure that model is learning and second is to make sure not to overfit by introducing regularizations etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Inputs\n",
    "\n",
    "Let $X = [x_1, x_2, ..., x_n] \\in R^{m \\times n}$ be our data, where $x_1, ...x_n$ are feature vectors. You can imagine that our data looks like this for $n = 2$:\n",
    "\n",
    "<img src=\"imgs/normalization1.png\">\n",
    "\n",
    "We first subtract the mean of each feature from $X - \\mu(X) = [x_1 - \\frac{1}{m}\\sum_{i = 1}^{m}x_{i, 1}, .. , x_n - \\frac{1}{m}\\sum_{i = 1}^{m}x_{i, n}]$ and by doing that we are centering our data:\n",
    "\n",
    "<img src=\"imgs/normalization2.png\">\n",
    "\n",
    "And then, devide by standard deviation $\\frac{X - \\mu(X)}{\\sigma^2(X)} = [\\frac{x_1 - \\frac{1}{m}\\sum_{i = 1}^{m}x_{i, 1}}{\\frac{1}{m}\\sum_{i = 1}^{m}x^2_{i, 1}}, .. , \\frac{x_n - \\frac{1}{m}\\sum_{i = 1}^{m}x_{i, n}}{\\frac{1}{m}\\sum_{i = 1}^{m}x^2_{i, n}}]$ where now variance of all features is equal to 1:\n",
    "\n",
    "<img src=\"imgs/normalization3.png\">\n",
    "\n",
    "__Variance__ measures how far a set of (random) numbers are spread out from their average value (mean). And __standard deviation__ is squared variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing and exploading gradients\n",
    "\n",
    "When training very deep neural network we can encounter problem where gradients are too small (vanishing) or too big (exploading) which makes the training difficult. Problem of vanishing / exploading gradients can be partialy solved by carefull weight initialization depending on what activation function are you using.\n",
    "\n",
    "### Weight Initialization in a Deep Network\n",
    "\n",
    "1) If you are using $ReLU$ weights should be initialized to (he-et-al initialization https://arxiv.org/abs/1502.01852) and what it does is basically setting the variance of $w$ to $\\frac{2}{n_l}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "layer_size = [5, 4]\n",
    "l = 1\n",
    "w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) If you are using $Tanh$ weights should be initialized to (Xavier initialization http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "layer_size = [5, 4]\n",
    "l = 1\n",
    "w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or this (Bengio initialization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "layer_size = [5, 4]\n",
    "l = 1\n",
    "w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l] + layer_size[l-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical aproximation of gradients\n",
    "\n",
    "Derviatives can be aproximated with $f'(\\theta) \\approx \\frac{f(\\theta + \\varepsilon) - f(\\theta - \\varepsilon)}{2\\varepsilon}$\n",
    "\n",
    "<img src=\"imgs/numerical_grad_aprox.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient checking\n",
    "\n",
    "Gradient checking can be of great help when implementing backpropagation. It can be done in a following way:\n",
    "1. copy all neural network parametars into a single vector and do the same thing for derviatives of each parametar\n",
    "2. iterate through all parameters and for each compute approximation of the derivative using approximation formula on previous slide\n",
    "3. compare vector of approximate derivatives with original vector of derivatives \n",
    "\n",
    "<img src=\"imgs/grad_check.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient checking implementation notes\n",
    "\n",
    "<img src=\"imgs/grad_check_notes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "__Batch gradient descent__ means that we are processing entire training set in single iteration (`batch_size = m`). \n",
    "\n",
    "__Mini-batch gradient descent__ means that we are processing `batch_size` number of samples in a single iteration where `batch_size` is less then `m`.\n",
    "\n",
    "__Stochastic gradient descent__ means that we are processing single sample in a single iteration (`batch_size = 1`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding mini-batch gradient descent\n",
    "\n",
    "<img src=\"imgs/mini_batch_gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentially weighted averages\n",
    "\n",
    "<img src=\"imgs/exponentially_weighted_avg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding exponentially weighted averages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning process\n",
    "\n",
    "* Don't use grid search for hyperparameter search. Instead, use random sampling over the range of hyperparameters.\n",
    "* Consider implementing zoom in of the region that gives the best results and then repeat the process with ranges set to values that are within region that gave the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice basic tutorial ... very basic though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
